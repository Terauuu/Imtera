[
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Linglu’s Project",
    "section": "",
    "text": "Project Summary\n\n\n\n\n\n\nAmy Ji(6009476), Tera Li(7545403), Brandon Su(5807961)\n\n\nJun 11, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linglu Li",
    "section": "",
    "text": "current\nOperation | Eastwest Bank\nMaster of business analytics | Rady school of management"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Last update 2024_06_01\nDownload PDF file."
  },
  {
    "objectID": "project/HW1/Summary-copy.html",
    "href": "project/HW1/Summary-copy.html",
    "title": "Project Summary",
    "section": "",
    "text": "1 Introduction and Data Description\nOur project aims to predict students’ performances in mathematics in Portugal using the “Student Performance” dataset from the UCI Machine Learning Repository: https://archive.ics.uci.edu/dataset/320/student+performance. This dataset includes 395 students’ grades and various other attributes from two schools in Portugal: Gabriel Pereira or Mousinho da Silveira. We have selected 13 attributes that we considered the most relevant for predicting the final term mathematics performance \\(G3\\). Here are the descriptions of each variables.\n\n\n\nDescription of Variables\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPStatus\nParent’s cohabitation status (T - living together or A - apart)\n\n\nMjob\nMother’s job, civil services, at home or other\n\n\nFjob\nFather’s job, civil services, at home or other\n\n\nAddress\nStudent’s home address type (U - urban or R - rural)\n\n\nReason\nReason to choose this school\n\n\nAge\nStudent’s age\n\n\nStudy_time\nWeekly study time: 1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - &gt;10 hours\n\n\nFamily_rel\nQuality of family relationships, from 1 - very bad to 5 - excellent\n\n\nWalc\nWeekend alcohol consumption, from 1 - very low to 5 - very high\n\n\nHealth\nCurrent health status, from 1 - very bad to 5 - very good\n\n\nAbsences\nNumber of school absences\n\n\nG1\nFirst term grade\n\n\nG2\nSecond term grade\n\n\nG3(Response)\nFinal term grade\n\n\n\n\n\nNow, we are going to apply several models to our data.\n\n\n2 Simple Linear Model\n\nThe relationship between the amount of absences and the G3(final quarter grade) should be linear.\nEach student’s amount of absences and \\(G3\\) should be independent of another’s.\nThe variance of residuals should be consistent across all levels of absences.\nThe residuals of the model should approximately follow a normal distribution.\nSince only one predictor (absences) is considered in relation to \\(G3\\) in our model, multicollinearity is not an issue.\nOur is:\n\\[\\large G3 \\sim \\beta_0 + \\beta_1 \\text{logAbsences}_i  + \\varepsilon_i\\] \\(\\beta_0\\): The intercept term, it represents the value of G3 when log(Absences) = 0 \\(\\beta_1\\): The coefficient of log(Absences) \\(\\varepsilon\\): The error term represents the difference between the observed value of \\(G3\\) and the value predicted by the regression line.\nFirst, let us exhibit the scatterplot of G3 and logAbsences.\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = G3 ~ logAbsences, data = mathdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4309 -2.2200  0.4132  3.1235  9.5691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.4309     0.3730  25.283  &lt; 2e-16 ***\nlogAbsences   0.7182     0.2156   3.331 0.000948 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.524 on 393 degrees of freedom\nMultiple R-squared:  0.02745,   Adjusted R-squared:  0.02498 \nF-statistic: 11.09 on 1 and 393 DF,  p-value: 0.000948\n\n\nFrom the p-value, we can see that both intercept and logAbsences exhibit significance for prediction, and the \\(R^2\\) is not so high, we may assume that one predictor is not enough for our model to predict the G3\n\n\n\n\n\n\n\n\n\nWe can see that the residuals indeed destributed alike Gaussian, but there are still some weird behaviors in the QQ plot, and verifies our thought that one predictor logAbsences may not be enough to predict the G3. Hence, we will move forward to the Multiple Linear Regression Model and set up more predictors for our dataset.\n\n\n3 Multiple Linear Model\nStarting from this part, we are going to separate our dataset into 70% of training and 30% of testing.\nFirst of all, we set up a whole model and then we use stepwise selection to crawl the model space, adding or removing variables one at a time until reaching a stoppping point, depending on which is better. Stepwise methods are combinations of forward selection and backward elimination, it can start at anywhere, at each iteration, either a forward or backward step can be taken. And we found the significant predictors depending on their p-values presented below.\n\n\n\nCall:\nlm(formula = G3 ~ ., data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3894 -0.6469  0.2466  1.1220  3.7178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.8243587  1.8307158   0.450  0.65288    \nAge         -0.2573852  0.0952697  -2.702  0.00736 ** \nStudy_time2 -0.2402154  0.2710217  -0.886  0.37626    \nStudy_time3 -0.1413436  0.3743374  -0.378  0.70605    \nStudy_time4 -0.6437278  0.4781429  -1.346  0.17938    \nFamily_rel2  0.3189716  0.9517935   0.335  0.73780    \nFamily_rel3  0.9124278  0.8164941   1.117  0.26482    \nFamily_rel4  1.0587478  0.7883582   1.343  0.18045    \nFamily_rel5  1.6731527  0.8028273   2.084  0.03813 *  \nWalc2       -0.3259161  0.3106404  -1.049  0.29508    \nWalc3        0.0449286  0.3144706   0.143  0.88650    \nWalc4        0.0005311  0.3766008   0.001  0.99888    \nWalc5        0.1503265  0.5040269   0.298  0.76575    \nHealth2     -0.5259303  0.4813733  -1.093  0.27560    \nHealth3      0.2370666  0.4150752   0.571  0.56840    \nHealth4     -0.1859810  0.4432203  -0.420  0.67512    \nHealth5      0.1809272  0.3809675   0.475  0.63525    \nG1           0.2015680  0.0675709   2.983  0.00313 ** \nG2           0.9229364  0.0586557  15.735  &lt; 2e-16 ***\nlogAbsences  0.6273482  0.1094159   5.734 2.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.845 on 259 degrees of freedom\nMultiple R-squared:  0.8555,    Adjusted R-squared:  0.8449 \nF-statistic: 80.69 on 19 and 259 DF,  p-value: &lt; 2.2e-16\n\n\nNew model : \\[\\text{G3}_i \\sim \\beta_1 \\text{Age}_i + \\beta_2 \\mathbf{1}\\{\\text{Family\\_rel}_i = 5\\} + \\beta_3 \\text{G1}_i + \\beta_4 \\text{G2}_i + \\beta_5 \\log(\\text{Absences}_i) + \\varepsilon_i\\] Interesting finding, in the simple linear regression step, we found that intercept was significant and then here, intercept was not significant, which is an surprising finding that deserves further investigation.\n\\[\\text{G3_tran}_i \\sim \\beta_1 \\text{Age}_i+ \\beta_2 \\text{G1}_i + \\beta_3 \\text{G2}_i + \\beta_4 \\log(\\text{Absences}_i) + \\varepsilon_i\\]\nApply the model to our testing data and see the residuals.\n\n\n\n\n\n\n\n\n\nThe more predictors we use, the higher chance we might encounter multi-collinearity within our predictors, and we can see from the ggpairs plot, it has shown that G1, G2, and G3 are highly correlated with each other, as demonstrated by the GGpairs plot. To address collinearity, we are going to employ shrinkage methods such as LASSO and Ridge Regression. Shrinkage techniques build on the principles from the mathematical optimization models of multiple linear regression by introducing a penalizing term to the loss function. These methods are typically used to address collinearity and aid in variable selection.\n\n\n4 Shrinkage Method\n\n\n$Ridge_Lambda\n[1] 0.4234572\n\n$LASSO_Lambda\n[1] 0.09337785\n\n\n\n\n\nCoefficients from Ridge and LASSO Models\n\n\nTerm\nRidge\nLASSO\n\n\n\n\n(Intercept)\n2.3438867\n1.1115948\n\n\nAge\n-0.2728467\n-0.1902027\n\n\nStudy_time2\n-0.1774214\n0.0000000\n\n\nStudy_time3\n-0.0641157\n0.0000000\n\n\nStudy_time4\n-0.4960853\n-0.0804816\n\n\nFamily_rel2\n-0.5399116\n-0.2885848\n\n\nFamily_rel3\n0.0889920\n0.0000000\n\n\nFamily_rel4\n0.1521927\n0.0000000\n\n\nFamily_rel5\n0.7330677\n0.4586829\n\n\nWalc2\n-0.4400705\n-0.1391771\n\n\nWalc3\n0.0925369\n0.0000000\n\n\nWalc4\n0.0412947\n0.0000000\n\n\nWalc5\n0.1554768\n0.0000000\n\n\nHealth2\n-0.5149864\n-0.3181407\n\n\nHealth3\n0.1148550\n0.0000000\n\n\nHealth4\n-0.2416216\n-0.0306104\n\n\nHealth5\n0.1234786\n0.0502768\n\n\nG1\n0.3515291\n0.1719111\n\n\nG2\n0.7396923\n0.9188512\n\n\nlogAbsences\n0.5850484\n0.5159241\n\n\n\n\n\nBy using them, we can see that some categorical variables are indeed important. Ridge Regression includes all predictors, shrinking their coefficients towards zero without eliminating them, resulting in smaller coefficients compared to MLR due to regularization. LASSO, on the other hand, performs more strict variable selection by shrinking the coefficients of insignificant predictors to \\(0\\), which is why it includes fewer predictors. Notably, both models include the intercept, which we previously ruled as insignificant in the MLR model. Despite these differences, we observe that the coefficients for Age, logAbsences, G1, and G2 are quite close to the MLR models, suggesting that these variables are consistently important predictors of G3. Now, we can apply these models for prediction and test which one provides better prediction accuracy.\n\n\n\nAccuracy Criteria\n\n\nModel\nR2\nRMSE\nMAE\n\n\n\n\nMLR\n0.8397116\n1.732140\n1.127933\n\n\nRidge\n0.8334957\n1.765406\n1.252630\n\n\nLASSO\n0.8439452\n1.709112\n1.093087\n\n\n\n\n\n\n\n5 Inovation\nThe methods we chose, the Box-Cox Transformation and the Bootstrap Method, were highly effective in improving our regression model’s accuracy and reliability.\nThe data’s non-linearity and heteroscedasticity were effectively addressed by the Box-Cox Transformation, which stabilized variance and improved the residuals’ normal distribution. This is demonstrated by the Q-Q plot’s more precise matching of residuals with the diagonal line and the Residuals vs. Fitted plot’s more randomly distributed residuals around zero.\nAdditionally, the Bootstrap Method validated the stability and reliability of our model’s coefficients by repeatedly resampling the data, which confirmed that the original coefficients were close to the bootstrap means and fell within the narrow bootstrap confidence intervals. The results indicate the great reliability and stability of our estimations, showing the accuracy and not a significant bias in the model’s coefficients.\nOverall, these methods provided significant insight into the variables that affect student performance in the final and ensured that our model’s predictions were accurate and reliable.\n\n\n6 Practical and Inferential Insights\nThis data analysis provides valuable insights for improving student outcomes and tailoring support strategies within the education system. By understanding the relationship between absences and grades, we can draw meaningful conclusions and make predictions about student performance.\nSchool Administration and Teachers:\nThe results help school administrators identify trends and areas of concern, enabling the development of policies and interventions to reduce absenteeism and boost academic achievement. Teachers can adjust instruction strategies, use flexible attendance policies, and provide additional resources to support students, using data-driven insights to target the most impactful factors.\nParents:\nParents gain a clearer understanding of how attendance affects academic performance, allowing them to encourage consistent attendance and address barriers. This knowledge fosters better collaboration with teachers and counselors to support their children’s educational journey.\n\n\n7 Conclusion\nIn general, we found that these three models have very similar metrics. Before conducting this analysis, we expected LASSO or Ridge to outperform MLR significantly due to their broader consideration of predictors. While all three models perform well, LASSO has a slight advantage in terms of accuracy. Ridge regression provides stability by addressing collinearity, though with a minor reduction in prediction accuracy.\nHowever, the MLR model is still regarded as a suitable model for analysis as it has fewer number of variables indicate less complexity, which would be helpful to prevent overfitting and improve generalization in our model, the only cost is a little prediction accuracy.\nWhile our analysis provides intriguing insights into the relationship between various attributes and students’ performances, the current sample’s limitations warrant caution. Our goal is to predict mathematics per- formance across Portugal, yet our sample includes only about 400 students from 2 schools. In the 2021/22 school year, 963 schools provided upper secondary education to 397,100 students across Portugal (Education Statistics 2021/2022, DGEEC/DSEE). This limited sample size may not fully capture the diverse factors influencing student performance across different regions and demographics.\nA more representative sample is crucial for enhancing the robustness and reliability of our models and predictions. Including a larger and more diverse group of students would ensure our analysis accounts for varying socio-economic backgrounds, educational resources, and regional differences. Moreover, a broader sample would enable us to identify new significant predictors and filter down existing ones more accurately.\n\n\nThoughts\nAs a concluding thought, our group found this topic particularly compelling. In today’s competitive educa- tional landscape, grades have become increasingly important to many people. Parents often face challenges in supporting their children’s education while striving to provide the best possible guidance. This study is especially meaningful as it can serve as an educational guide for new parents who are deeply invested in their children’s academic success. By understanding the factors that influence student performance, they can better support and encourage their children’s educational journeys. Our work with regression techniques has inspired us to apply them to other important datasets. Beyond educational data, we could analyze world poverty and famine rates to uncover insights that inform policy decisions. Understanding the factors behind these issues will enhance our skills and help us make a meaningful impact. By extending our use of regression techniques, we aim to contribute positively to addressing global challenges."
  }
]