[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan An",
    "section": "",
    "text": "current\nOperation | Eastwest Bank\nMaster of business analytics | Rady school of management"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Ethan’s project",
    "section": "",
    "text": "Homework 2: Poisson Regression Examples\n\n\n\n\n\n\nEthan An\n\n\nJun 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: Multinomial Logit Model\n\n\n\n\n\n\nEthan An\n\n\nJun 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: A Replication of Karlan and List (2007)\n\n\n\n\n\n\nEthan An\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Last update 2024_06_01\nDownload PDF file."
  },
  {
    "objectID": "project/hw1/hw1_questions.html",
    "href": "project/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nTo better understand how pricing mechanisms affect charitable giving, Karlan and List implemented a large-scale natural field experiment involving over 50,000 prior donors to a politically active nonprofit organization. Participants were randomly assigned to one of several groups:\n\nA control group, receiving a standard fundraising letter with no mention of a match.\nOne or more treatment groups, receiving letters offering a matching grant of varying ratios—$1:$1, $2:$1, or $3:$1.\nEach treatment also varied the maximum matching amount (e.g., $25,000 or $100,000), and suggested donation levels (based on prior donations).\n\nThis experimental design allowed the researchers to isolate the causal effect of matching offers—both their presence and their generosity—on the likelihood of donating and the amount donated. It also enabled an exploration of how political context might moderate these effects.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/hw1_questions.html#introduction",
    "href": "project/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nTo better understand how pricing mechanisms affect charitable giving, Karlan and List implemented a large-scale natural field experiment involving over 50,000 prior donors to a politically active nonprofit organization. Participants were randomly assigned to one of several groups:\n\nA control group, receiving a standard fundraising letter with no mention of a match.\nOne or more treatment groups, receiving letters offering a matching grant of varying ratios—$1:$1, $2:$1, or $3:$1.\nEach treatment also varied the maximum matching amount (e.g., $25,000 or $100,000), and suggested donation levels (based on prior donations).\n\nThis experimental design allowed the researchers to isolate the causal effect of matching offers—both their presence and their generosity—on the likelihood of donating and the amount donated. It also enabled an exploration of how political context might moderate these effects.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/hw1_questions.html#data",
    "href": "project/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\nWe load the dataset provided by Karlan and List (2007), which contains responses from over 50,000 previous donors who were randomly assigned to receive different versions of a fundraising letter. The dataset includes treatment assignments (e.g., control vs. matching grant groups), donation outcomes, and demographic/political context data.\nThe data has 50,083 observations and 51 variables. Some of the key variables include:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nfrom scipy import stats\nimport pandas as pd\n\ntreated = df[df[\"treatment\"] == 1]\ncontrol = df[df[\"control\"] == 1]\n\nvars_to_test = [\"years\", \"freq\", \"mrm2\", \"female\", \"couple\"]\nbalance_table = []\n\nfor var in vars_to_test:\n    t_mean = treated[var].mean()\n    c_mean = control[var].mean()\n    t_stat, p_val = stats.ttest_ind(treated[var].dropna(), control[var].dropna(), equal_var=False)\n    balance_table.append((var, round(c_mean, 2), round(t_mean, 2), round(p_val, 4)))\n\npd.DataFrame(balance_table, columns=[\"Variable\", \"Control Mean\", \"Treatment Mean\", \"p-value\"])\n\n\n\n\n\n\n\nVariable\nControl Mean\nTreatment Mean\np-value\n\n\n\n\n0\nyears\n6.14\n6.08\n0.2753\n\n\n1\nfreq\n8.05\n8.04\n0.9117\n\n\n2\nmrm2\n13.00\n13.01\n0.9049\n\n\n3\nfemale\n0.28\n0.28\n0.0795\n\n\n4\ncouple\n0.09\n0.09\n0.5604\n\n\n\n\n\nBased on the p-values, we observe that none of the variables differ significantly between the two groups at the 5% level. This supports the notion that random assignment was successful and that the treatment and control groups are statistically balanced at baseline.\nTo confirm this, we also fit a simple linear regression model: ### Regression: Effect of Treatment on Prior Giving Behavior\nWe use a linear regression to test whether prior donation behavior (mrm2) differs across treatment groups. This serves as a balance check for the experimental design.\n\nimport pyrsm as rsm\n\nreg = rsm.model.regress(\n    data={\"df\": df},\n    rvar=\"mrm2\",\n    evar=\"treatment\"\n)\n\nreg.summary()\n\nThe regression output is summarized in the following screenshot:\n\n\n\nRegression output\n\n\nThe treatment coefficient is small (0.014) and statistically insignificant (p = 0.905), indicating that the treatment and control groups were well balanced in terms of prior maximum donations.\nThis supports the validity of the random assignment mechanism."
  },
  {
    "objectID": "project/hw1/hw1_questions.html#experimental-results",
    "href": "project/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\ndonation_rate.plot(kind=\"bar\", color=[\"gray\", \"steelblue\"], edgecolor=\"black\")\nplt.title(\"Donation Rate by Treatment Group\")\nplt.ylabel(\"Proportion Donated\")\nplt.ylim(0, 0.03)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\ntreated = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\nprint(f\"T-statistic = {t_stat:.3f}, p-value = {p_val:.4f}\")\n\nT-statistic = 3.209, p-value = 0.0013\n\n\n\nimport pyrsm as rsm\n\nreg = rsm.model.regress(\n    data={\"df\": df},\n    rvar=\"gave\",\n    evar=\"treatment\"\n)\nreg.summary()\n\n\n\n\nRegression output\n\n\nThe results above show that the treatment group had a statistically significantly higher probability of making a donation compared to the control group.\n\nThe bar chart illustrates a clear increase in donation rate for the treatment group.\nA two-sample t-test confirms this difference is statistically significant (p = 0.0013).\nA linear regression further supports this finding, with the treatment coefficient estimated at 0.004 (p = 0.002).\n\nThese results suggest that matched donation offers increased individuals’ likelihood of contributing. In behavioral terms, this implies people respond positively to incentives that enhance the perceived value of their contributions—highlighting the effectiveness of “matching” as a behavioral nudge.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nT-test Comparisons\n\nfrom scipy.stats import ttest_ind\n\ngave_1to1 = df[df[\"ratio2\"] == 0][df[\"ratio3\"] == 0][\"gave\"]\ngave_2to1 = df[df[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df[df[\"ratio3\"] == 1][\"gave\"]\n\n# t-test: 1:1 vs 2:1\ntstat_12, pval_12 = ttest_ind(gave_1to1, gave_2to1, equal_var=False)\n\n# t-test: 2:1 vs 3:1\ntstat_23, pval_23 = ttest_ind(gave_2to1, gave_3to1, equal_var=False)\n\nprint(f\"1:1 vs 2:1 -&gt; t = {tstat_12:.3f}, p = {pval_12:.4f}\")\nprint(f\"2:1 vs 3:1 -&gt; t = {tstat_23:.3f}, p = {pval_23:.4f}\")\n\n1:1 vs 2:1 -&gt; t = -2.220, p = 0.0265\n2:1 vs 3:1 -&gt; t = -0.050, p = 0.9600\n\n\n/tmp/ipykernel_3432/3218956970.py:3: UserWarning:\n\nBoolean Series key will be reindexed to match DataFrame index.\n\n\n\n\nimport pyrsm as rsm\n\nreg = rsm.model.regress(\n    data={\"df\": df},\n    rvar=\"gave\",\n    evar=[\"ratio2\", \"ratio3\"]\n)\n\nreg.summary()\n\nLinear regression (OLS)\nData                 : df\nResponse variable    : gave\nExplanatory variables: ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.019      0.001   22.306  &lt; .001  ***\nratio2           0.004      0.002    2.269   0.023    *\nratio3           0.004      0.002    2.332    0.02    *\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 4.117 df(2, 50080), p.value 0.016\nNr obs: 50,083\n\n\n\n\n\nInterpretation\nThe goal of this analysis was to evaluate whether increasing the generosity of the match (e.g., from 1:1 to 2:1 or 3:1) leads to higher donation rates.\nThe t-test results show that: - Moving from a 1:1 match to a 2:1 match results in a statistically significant increase in donations (p = 0.0265). - However, moving from a 2:1 to a 3:1 match shows no significant difference (p = 0.96), suggesting diminishing returns.\nThe linear regression results confirm this pattern: - Both ratio2 and ratio3 are associated with a significant increase in donation likelihood relative to the 1:1 baseline. - However, the effect sizes are identical (0.004), which suggests that the existence of a match matters more than its magnitude.\nFrom a behavioral perspective, this implies that: - Donors respond positively to the idea of their contribution being matched—it feels more “impactful.” - But increasing the match beyond a certain point does not make giving feel significantly better, nor does it increase motivation.\nIn short, the presence of a match drives behavior, not the size of the multiplier.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport pyrsm as rsm\n\nreg = rsm.model.regress(\n    data={\"df\": df},\n    rvar=\"amount\",\n    evar=\"treatment\"\n)\nreg.summary()\n\nLinear regression (OLS)\nData                 : df\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\n\ndonated_df = df[df[\"gave\"] == 1]\n\nreg2 = rsm.model.regress(\n    data={\"df\": donated_df},\n    rvar=\"amount\",\n    evar=\"treatment\"\n)\nreg2.summary()\n\nLinear regression (OLS)\nData                 : df\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\n\nimport matplotlib.pyplot as plt\n\ndonated = df[df[\"gave\"] == 1]\ncontrol = donated[donated[\"treatment\"] == 0][\"amount\"]\ntreatment = donated[donated[\"treatment\"] == 1][\"amount\"]\n\nplt.hist(control, bins=30, alpha=0.6, label=\"Control\", color=\"gray\", edgecolor=\"black\")\nplt.axvline(control.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean Control: ${control.mean():.2f}\")\n\nplt.hist(treatment, bins=30, alpha=0.6, label=\"Treatment\", color=\"steelblue\", edgecolor=\"black\")\nplt.axvline(treatment.mean(), color=\"blue\", linestyle=\"--\", label=f\"Mean Treatment: ${treatment.mean():.2f}\")\n\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Donation Amounts (Given Donation)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInterpretation\nWe assess whether the treatment affects the size of donations, not just the likelihood of giving.\n\nIn the full sample, a regression of amount ~ treatment shows no significant effect of the treatment on donation amount (including zeros).\nHowever, when limiting to individuals who actually donated, the conditional regression shows whether treatment increases the average size of the donation.\nFrom the histogram, we see that the distributions of donation amounts are relatively similar between groups, but the average donation is slightly higher in the treatment group.\n\nThese results suggest that while matched donations increase the likelihood of donating, they do not substantially alter the amount people choose to give once they’ve decided to donate. This implies that matching functions primarily as a participation nudge rather than a motivation to give more."
  },
  {
    "objectID": "project/hw1/hw1_questions.html#simulation-experiment",
    "href": "project/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_control = 0.018\np_treatment = 0.022\nn = 10000\n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control\n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure()\nplt.plot(cumulative_avg, label=\"Cumulative Avg. of Differences\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Average Treatment Effect\")\nplt.title(\"Law of Large Numbers in Action\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThis simulation visually demonstrates the Law of Large Numbers in the context of charitable donation data.\n\nInitially, the sample average of the difference between treatment and control is quite noisy and volatile.\nAs the number of observations increases, the cumulative average converges to the true treatment effect (0.004).\nThis supports the idea that with a large enough sample, sample-based statistics become reliable estimates of population parameters.\n\nThis experiment also reinforces why larger sample sizes are crucial for statistical inference, and why the t-statistic is more trustworthy when n is large.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_sim = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n\n    axes[i].hist(diffs, bins=30, color=\"skyblue\", edgecolor=\"black\", density=True)\n    axes[i].axvline(np.mean(diffs), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(diffs):.4f}\")\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Sample Mean Difference\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()\n\nplt.suptitle(\"CLT Simulation: Distribution of Sample Mean Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThis simulation demonstrates the Central Limit Theorem (CLT) by showing how the distribution of average treatment effects evolves with increasing sample size.\nEach histogram represents 1,000 simulations of the difference in means between treatment and control groups, at different sample sizes.\n\nAt n = 50, the distribution is jagged and skewed—far from normal.\nAs sample size increases (to 200, 500, and 1000), the distribution becomes smoother and more bell-shaped, eventually closely approximating a normal distribution.\n\nThis illustrates the CLT in action: regardless of the underlying distribution (Bernoulli here), the distribution of the sample mean tends toward normality as n increases. This justifies using t-tests and linear models for inference in randomized experiments like this one.\n\n\nInterpretation\nThis simulation illustrates the Central Limit Theorem (CLT) using differences in sample means between treatment and control groups.\nEach panel shows the distribution of 1,000 simulated average treatment effects (ATEs) at different sample sizes:\n\nAt n = 50, the distribution is irregular and spiky, showing high variability.\nAt n = 200, the distribution begins to smooth out, but is still visibly skewed.\nAt n = 500, it begins to resemble a normal distribution.\nAt n = 1000, the distribution is approximately bell-shaped and symmetric.\n\nThis visual progression confirms the CLT:\n&gt; As sample size increases, the distribution of the sample mean approaches a normal distribution, regardless of the underlying population distribution (Bernoulli in this case).\nThis is why classical statistical tools like t-tests are valid for large enough samples—even when the data is binary or skewed at the individual level."
  },
  {
    "objectID": "project/hw1/work flow.html",
    "href": "project/hw1/work flow.html",
    "title": "Home",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\ndf_info = df.info()\ndf_head = df.head()\n\ndf_info, df_head\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n(None,\n    treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n 0          0        1  Control       0       0   Control       0       0   \n 1          0        1  Control       0       0   Control       0       0   \n 2          1        0        1       0       0  $100,000       0       0   \n 3          1        0        1       0       0  Unstated       0       0   \n 4          1        0        1       0       0   $50,000       0       1   \n \n    size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n 0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n 1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n 2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n 3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n 4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n \n    ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n 0       2.10          28517.0  0.499807      0.324528            1.0  \n 1        NaN              NaN       NaN           NaN            NaN  \n 2       2.48          51175.0  0.721941      0.192668            1.0  \n 3       2.65          79269.0  0.920431      0.412142            1.0  \n 4       1.85          40908.0  0.416072      0.439965            1.0  \n \n [5 rows x 51 columns])"
  },
  {
    "objectID": "project/hw2/hw2_questions.html",
    "href": "project/hw2/hw2_questions.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=30, multiple=\"dodge\", palette=\"Set2\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\nmeans = df.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents:\")\nprint(\"  Non-Customers:\", round(means[0], 2))\nprint(\"  Customers:\", round(means[1], 2))\n Mean number of patents: Non-Customers: 3.47 Customers: 4.13\nWe observe that Blueprinty customers tend to have more patents on average than non-customers. The distribution of patent counts is right-skewed for both groups, but customers are more concentrated in the higher patent range (e.g., 5 or more patents).\n\n\n\n\nMean patents (Customers): 4.13\n\nMean patents (Non-Customers): 3.47\n\nThis suggests a potential positive association between using Blueprinty’s software and patenting success. However, we cannot infer causality from this descriptive comparison alone.\nIt is possible that firms more active in patenting are also more likely to adopt Blueprinty’s software. Therefore, it is important to control for potential confounding variables such as firm age and regional location in the regression analysis that follows.\nregion_table = df.groupby(\"iscustomer\")[\"region\"].value_counts(normalize=True).unstack().round(3)\nregion_table \n\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n0\n0.184\n0.268\n0.155\n0.153\n0.240\n\n\n1\n0.077\n0.682\n0.060\n0.073\n0.108\n\n\n\n\n\nage_table = df.groupby(\"iscustomer\")[\"age\"].mean().round(2)\nage_table\niscustomer\n0    26.1\n1    26.9\nName: age, dtype: float64\nWe observe notable differences in region distribution and firm age between Blueprinty customers and non-customers:\n\nRegion:\n\nAmong customers, ~68% are from the Northeast, with fewer in the Southwest (~11%), Midwest (~8%), South (~7%), and Northwest (~6%).\nAmong non-customers, the Northeast accounts for only ~27%, with larger proportions in the Southwest (~24%), Midwest (~18%), South (~15%), and Northwest (~15%).\n\nAverage Firm Age:\n\nCustomers: 26.90 years\n\nNon-customers: 26.10 years\n\n\nThese patterns suggest that customer status is not random and must be accounted for in any causal inference. We’ll include both region and age as covariates in our regression model.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nFor a sample of ( n ) independent observations, the likelihood function is:\n\\[S\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood (log-likelihood), we get:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nfrom scipy.special import gammaln  # gammaln(y+1) = log(y!)\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -1e6\n    loglik = np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n    return loglik\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -1e6\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\nlambda_grid = np.linspace(0.1, 10, 200)\n\nloglik_vals = [poisson_loglikelihood(l, Y) for l in lambda_grid]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_grid, loglik_vals, label=\"Log-Likelihood\")\nplt.xlabel(r\"$\\lambda$\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nfrom scipy.optimize import minimize_scalar\n\nY = df[\"patents\"].values\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.1, 10), \n    method='bounded'\n)\n\nprint(\"Estimated λ (MLE):\", round(result.x, 4))\nprint(\"Maximum Log-Likelihood:\", round(-result.fun, 2))\nEstimated λ (MLE): 3.6847 Maximum Log-Likelihood: -3367.68\n\n\n\nUsing our hand-coded Poisson log-likelihood function and the scipy.optimize.minimize_scalar() method,\nwe estimated the average number of patents per firm (λ) that best fits the data.\n\nEstimated λ (MLE): 3.6847\n\nMaximum Log-Likelihood: -3367.68\n\nThis estimate maximizes the log-likelihood of observing the actual distribution of patents across firms,\nassuming a Poisson model. This λ will serve as the baseline for further regression modeling.\nWe now derive the maximum likelihood estimator (MLE) for λ analytically.\nRecall the log-likelihood function for the Poisson distribution:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative with respect to λ:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to 0 and solve:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThus, the MLE for λ is simply the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean equals the rate parameter (λ),\nso the best estimate of λ is just the average observed number of patents.\nfrom scipy.optimize import minimize_scalar\n\nY = df[\"patents\"].values\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.1, 10),\n    method='bounded'\n)\n\nprint(\"Estimated λ (MLE):\", round(result.x, 4))\nprint(\"Maximum Log-Likelihood:\", round(-result.fun, 2))\nEstimated λ (MLE): 3.6847 Maximum Log-Likelihood: -3367.68\nWe computed the MLE for λ by numerically optimizing our hand-coded log-likelihood function using\nscipy.optimize.minimize_scalar() in Python. The optimizer returned the following result:\n\nEstimated λ (MLE): 3.6847\n\nMaximum Log-Likelihood: -3367.68\n\nThis aligns with the theoretical result that the MLE of λ for a Poisson distribution is the sample mean.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\nX = df[[\"age\"]].copy()\nX[\"age_sq\"] = df[\"age\"] ** 2\nX = pd.get_dummies(X.join(df[\"region\"]), drop_first=True)\nX[\"iscustomer\"] = df[\"iscustomer\"]\nX = sm.add_constant(X)\n\nX_mat = X.to_numpy(dtype=float)\nY = df[\"patents\"].to_numpy(dtype=int)\n\ndef poisson_loglikelihood(beta, X, Y):\n    beta = np.asarray(beta)\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)             \n    lambda_i = np.exp(eta)\n    loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return -loglik\n\ninit_beta = np.zeros(X_mat.shape[1])\n\nresult = minimize(\n    fun=poisson_loglikelihood,\n    x0=init_beta,\n    args=(X_mat, Y),\n    method=\"BFGS\",\n    options={\"disp\": True}\n)\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv \nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": np.round(beta_hat, 4),\n    \"Std. Error\": np.round(standard_errors, 4)\n})\n\nprint(coef_table)\n     Current function value: 3258.072164\n     Iterations: 14\n     Function evaluations: 759\n     Gradient evaluations: 83\n       Variable  Estimate  Std. Error\n0 const -0.5100 0.1930 1 age 0.1487 0.0145 2 age_sq -0.0030 0.0003 3 region_Northeast 0.0292 0.0468 4 region_Northwest -0.0176 0.0572 5 region_South 0.0566 0.0562 6 region_Southwest 0.0506 0.0496 7 iscustomer 0.2076 0.0329\n/opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:708: OptimizeWarning:\n\nDesired error not necessarily achieved due to precision loss.\n\n\n\n\nWe estimated a Poisson regression model using maximum likelihood, where the log of the expected number of patents per firm is modeled as a linear function of firm characteristics.\nThe results indicate that:\n\nAge is positively associated with patent counts, but with a diminishing return (age squared is negative and significant).\nBlueprinty customers file significantly more patents, on average. The coefficient on iscustomer is +0.2076, with a standard error of 0.0329, indicating a statistically significant effect.\nRegion effects are small and mostly not statistically significant relative to the omitted base category.\n\nExponentiating the iscustomer coefficient gives:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis suggests that, holding other variables constant, Blueprinty customers are expected to have ~23% more patents than non-customers.\n\nimport statsmodels.api as sm\n\nX_fixed = X.astype(\"float\")\n\n\nglm_poisson = sm.GLM(df[\"patents\"], X_fixed, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\nprint(glm_results.summary())\n\n\n\n\nThe following shows the fitted Poisson regression results using Python’s built-in GLM() function:\n\n\n\n\nThe Poisson regression model estimates the expected number of patents awarded to a firm as a function of its characteristics. The key findings are:\n\nAge has a positive and statistically significant coefficient (0.1486), meaning that older firms tend to file more patents.\nHowever, the negative coefficient on age squared (-0.0030) indicates diminishing returns — the relationship is concave.\nRegion indicators are not statistically significant at the 5% level, suggesting little difference in patenting behavior across regions once other factors are controlled for.\nMost importantly, the coefficient on iscustomer is 0.2076 and highly significant (p &lt; 0.001).\nThis implies that, holding other factors constant, Blueprinty customers are associated with higher expected patent counts.\nSpecifically, exponentiating the coefficient gives:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nMeaning Blueprinty customers are expected to file approximately 23% more patents than non-customers, all else equal.\n\nThis result supports the claim that using Blueprinty’s software is associated with greater patenting activity.\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_0 = X_0.astype(float) \n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\nX_1 = X_1.astype(float)\n\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\navg_diff = (y_pred_1 - y_pred_0).mean()\nprint(\"Average predicted patent increase due to Blueprinty:\", round(avg_diff, 4))\nAverage predicted patent increase due to Blueprinty: 0.7928\n\n\n\nTo better understand the impact of Blueprinty software, we simulated a counterfactual scenario:\n\nWe predicted the number of patents for each firm assuming they do not use Blueprinty (set iscustomer = 0)\nThen we predicted the number assuming all firms use Blueprinty (iscustomer = 1)\n\nBy taking the difference in predicted patent counts for each firm and averaging, we estimate the average treatment effect.\nResult:\nIf all firms were Blueprinty customers, the average predicted number of patents would increase by 0.7928.\nThis suggests that Blueprinty’s software is associated with a meaningful increase in patent productivity, even after controlling for firm age and region."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "project/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=30, multiple=\"dodge\", palette=\"Set2\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\nmeans = df.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents:\")\nprint(\"  Non-Customers:\", round(means[0], 2))\nprint(\"  Customers:\", round(means[1], 2))\n Mean number of patents: Non-Customers: 3.47 Customers: 4.13\nWe observe that Blueprinty customers tend to have more patents on average than non-customers. The distribution of patent counts is right-skewed for both groups, but customers are more concentrated in the higher patent range (e.g., 5 or more patents).\n\n\n\n\nMean patents (Customers): 4.13\n\nMean patents (Non-Customers): 3.47\n\nThis suggests a potential positive association between using Blueprinty’s software and patenting success. However, we cannot infer causality from this descriptive comparison alone.\nIt is possible that firms more active in patenting are also more likely to adopt Blueprinty’s software. Therefore, it is important to control for potential confounding variables such as firm age and regional location in the regression analysis that follows.\nregion_table = df.groupby(\"iscustomer\")[\"region\"].value_counts(normalize=True).unstack().round(3)\nregion_table \n\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n0\n0.184\n0.268\n0.155\n0.153\n0.240\n\n\n1\n0.077\n0.682\n0.060\n0.073\n0.108\n\n\n\n\n\nage_table = df.groupby(\"iscustomer\")[\"age\"].mean().round(2)\nage_table\niscustomer\n0    26.1\n1    26.9\nName: age, dtype: float64\nWe observe notable differences in region distribution and firm age between Blueprinty customers and non-customers:\n\nRegion:\n\nAmong customers, ~68% are from the Northeast, with fewer in the Southwest (~11%), Midwest (~8%), South (~7%), and Northwest (~6%).\nAmong non-customers, the Northeast accounts for only ~27%, with larger proportions in the Southwest (~24%), Midwest (~18%), South (~15%), and Northwest (~15%).\n\nAverage Firm Age:\n\nCustomers: 26.90 years\n\nNon-customers: 26.10 years\n\n\nThese patterns suggest that customer status is not random and must be accounted for in any causal inference. We’ll include both region and age as covariates in our regression model.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nFor a sample of ( n ) independent observations, the likelihood function is:\n\\[S\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood (log-likelihood), we get:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nfrom scipy.special import gammaln  # gammaln(y+1) = log(y!)\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -1e6\n    loglik = np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n    return loglik\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -1e6\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\nlambda_grid = np.linspace(0.1, 10, 200)\n\nloglik_vals = [poisson_loglikelihood(l, Y) for l in lambda_grid]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_grid, loglik_vals, label=\"Log-Likelihood\")\nplt.xlabel(r\"$\\lambda$\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nfrom scipy.optimize import minimize_scalar\n\nY = df[\"patents\"].values\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.1, 10), \n    method='bounded'\n)\n\nprint(\"Estimated λ (MLE):\", round(result.x, 4))\nprint(\"Maximum Log-Likelihood:\", round(-result.fun, 2))\nEstimated λ (MLE): 3.6847 Maximum Log-Likelihood: -3367.68\n\n\n\nUsing our hand-coded Poisson log-likelihood function and the scipy.optimize.minimize_scalar() method,\nwe estimated the average number of patents per firm (λ) that best fits the data.\n\nEstimated λ (MLE): 3.6847\n\nMaximum Log-Likelihood: -3367.68\n\nThis estimate maximizes the log-likelihood of observing the actual distribution of patents across firms,\nassuming a Poisson model. This λ will serve as the baseline for further regression modeling.\nWe now derive the maximum likelihood estimator (MLE) for λ analytically.\nRecall the log-likelihood function for the Poisson distribution:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative with respect to λ:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to 0 and solve:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThus, the MLE for λ is simply the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean equals the rate parameter (λ),\nso the best estimate of λ is just the average observed number of patents.\nfrom scipy.optimize import minimize_scalar\n\nY = df[\"patents\"].values\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.1, 10),\n    method='bounded'\n)\n\nprint(\"Estimated λ (MLE):\", round(result.x, 4))\nprint(\"Maximum Log-Likelihood:\", round(-result.fun, 2))\nEstimated λ (MLE): 3.6847 Maximum Log-Likelihood: -3367.68\nWe computed the MLE for λ by numerically optimizing our hand-coded log-likelihood function using\nscipy.optimize.minimize_scalar() in Python. The optimizer returned the following result:\n\nEstimated λ (MLE): 3.6847\n\nMaximum Log-Likelihood: -3367.68\n\nThis aligns with the theoretical result that the MLE of λ for a Poisson distribution is the sample mean.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\nX = df[[\"age\"]].copy()\nX[\"age_sq\"] = df[\"age\"] ** 2\nX = pd.get_dummies(X.join(df[\"region\"]), drop_first=True)\nX[\"iscustomer\"] = df[\"iscustomer\"]\nX = sm.add_constant(X)\n\nX_mat = X.to_numpy(dtype=float)\nY = df[\"patents\"].to_numpy(dtype=int)\n\ndef poisson_loglikelihood(beta, X, Y):\n    beta = np.asarray(beta)\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)             \n    lambda_i = np.exp(eta)\n    loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return -loglik\n\ninit_beta = np.zeros(X_mat.shape[1])\n\nresult = minimize(\n    fun=poisson_loglikelihood,\n    x0=init_beta,\n    args=(X_mat, Y),\n    method=\"BFGS\",\n    options={\"disp\": True}\n)\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv \nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": np.round(beta_hat, 4),\n    \"Std. Error\": np.round(standard_errors, 4)\n})\n\nprint(coef_table)\n     Current function value: 3258.072164\n     Iterations: 14\n     Function evaluations: 759\n     Gradient evaluations: 83\n       Variable  Estimate  Std. Error\n0 const -0.5100 0.1930 1 age 0.1487 0.0145 2 age_sq -0.0030 0.0003 3 region_Northeast 0.0292 0.0468 4 region_Northwest -0.0176 0.0572 5 region_South 0.0566 0.0562 6 region_Southwest 0.0506 0.0496 7 iscustomer 0.2076 0.0329\n/opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:708: OptimizeWarning:\n\nDesired error not necessarily achieved due to precision loss.\n\n\n\n\nWe estimated a Poisson regression model using maximum likelihood, where the log of the expected number of patents per firm is modeled as a linear function of firm characteristics.\nThe results indicate that:\n\nAge is positively associated with patent counts, but with a diminishing return (age squared is negative and significant).\nBlueprinty customers file significantly more patents, on average. The coefficient on iscustomer is +0.2076, with a standard error of 0.0329, indicating a statistically significant effect.\nRegion effects are small and mostly not statistically significant relative to the omitted base category.\n\nExponentiating the iscustomer coefficient gives:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis suggests that, holding other variables constant, Blueprinty customers are expected to have ~23% more patents than non-customers.\n\nimport statsmodels.api as sm\n\nX_fixed = X.astype(\"float\")\n\n\nglm_poisson = sm.GLM(df[\"patents\"], X_fixed, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\nprint(glm_results.summary())\n\n\n\n\nThe following shows the fitted Poisson regression results using Python’s built-in GLM() function:\n\n\n\n\nThe Poisson regression model estimates the expected number of patents awarded to a firm as a function of its characteristics. The key findings are:\n\nAge has a positive and statistically significant coefficient (0.1486), meaning that older firms tend to file more patents.\nHowever, the negative coefficient on age squared (-0.0030) indicates diminishing returns — the relationship is concave.\nRegion indicators are not statistically significant at the 5% level, suggesting little difference in patenting behavior across regions once other factors are controlled for.\nMost importantly, the coefficient on iscustomer is 0.2076 and highly significant (p &lt; 0.001).\nThis implies that, holding other factors constant, Blueprinty customers are associated with higher expected patent counts.\nSpecifically, exponentiating the coefficient gives:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nMeaning Blueprinty customers are expected to file approximately 23% more patents than non-customers, all else equal.\n\nThis result supports the claim that using Blueprinty’s software is associated with greater patenting activity.\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_0 = X_0.astype(float) \n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\nX_1 = X_1.astype(float)\n\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\navg_diff = (y_pred_1 - y_pred_0).mean()\nprint(\"Average predicted patent increase due to Blueprinty:\", round(avg_diff, 4))\nAverage predicted patent increase due to Blueprinty: 0.7928\n\n\n\nTo better understand the impact of Blueprinty software, we simulated a counterfactual scenario:\n\nWe predicted the number of patents for each firm assuming they do not use Blueprinty (set iscustomer = 0)\nThen we predicted the number assuming all firms use Blueprinty (iscustomer = 1)\n\nBy taking the difference in predicted patent counts for each firm and averaging, we estimate the average treatment effect.\nResult:\nIf all firms were Blueprinty customers, the average predicted number of patents would increase by 0.7928.\nThis suggests that Blueprinty’s software is associated with a meaningful increase in patent productivity, even after controlling for firm age and region."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#airbnb-case-study",
    "href": "project/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not"
  },
  {
    "objectID": "project/hw2/hw2_questions.html#data-reading-and-cleaning",
    "href": "project/hw2/hw2_questions.html#data-reading-and-cleaning",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "1. Data Reading and Cleaning",
    "text": "1. Data Reading and Cleaning\n\nimport pandas as pd\n\ndf_airbnb = pd.read_csv(\"airbnb.csv\")\n\nbasic_info = df_airbnb.info()\nmissing_summary = df_airbnb.isnull().sum()\n\nhead_preview = df_airbnb.head()\n\nbasic_info_str = str(basic_info)\nmissing_summary_str = str(missing_summary)\nhead_preview_str = str(head_preview)\n\n(basic_info_str, missing_summary_str, head_preview_str)\n\ndf = pd.read_csv(\"airbnb.csv\")\n\nvars_required = [\n    \"number_of_reviews\",\n    \"price\",\n    \"room_type\",\n    \"bathrooms\",\n    \"bedrooms\",\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n    \"instant_bookable\"\n]\n\ndf_clean = df[vars_required].dropna()\n\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\nclean_shape = df_clean.shape\nmissing_check = df_clean.isnull().sum()\npreview = df_clean.head()\n\n(clean_shape, missing_check, preview)\n((30160, 9),\n number_of_reviews            0\n price                        0\n room_type                    0\n bathrooms                    0\n bedrooms                     0\n review_scores_cleanliness    0\n review_scores_location       0\n review_scores_value          0\n instant_bookable             0\n dtype: int64,\n    number_of_reviews  price        room_type  bathrooms  bedrooms  \\\n 0                150     59     Private room        1.0       1.0   \n 1                 20    230  Entire home/apt        1.0       0.0   \n 3                116     89  Entire home/apt        1.0       1.0   \n 5                 60    212  Entire home/apt        1.0       1.0   \n 6                 60    250  Entire home/apt        1.0       2.0   \n \n    review_scores_cleanliness  review_scores_location  review_scores_value  \\\n 0                        9.0                     9.0                  9.0   \n 1                        9.0                    10.0                  9.0   \n 3                        9.0                     9.0                  9.0   \n 5                        9.0                     9.0                  9.0   \n 6                       10.0                     9.0                 10.0   \n \n    instant_bookable  \n 0                 0  \n 1                 0  \n 3                 0  \n 5                 0  \n 6                 0  )\n\n🧹 Data Cleaning Summary\nBefore modeling, we performed basic data cleaning to ensure a consistent and usable dataset. Specifically:\nWe focused on variables relevant for predicting the number of reviews:\nnumber_of_reviews, price, room_type, bathrooms, bedrooms, review_scores_cleanliness, review_scores_location, review_scores_value, instant_bookable\nWe dropped rows with missing values in any of these variables, resulting in a cleaned dataset with 30,160 observations.\nThe variable instant_bookable was originally a string (“t”/“f”), and we converted it to a binary numeric variable (1/0).\nAll variables now have appropriate types for modeling:\nNumeric variables are of type float or int\nroom_type remains as a categorical variable, to be converted to dummy variables for regression\nThis cleaned dataset is now ready for exploratory data analysis and Poisson regression modeling."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#exploratory-data-analysis",
    "href": "project/hw2/hw2_questions.html#exploratory-data-analysis",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "2. Exploratory Data Analysis",
    "text": "2. Exploratory Data Analysis\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df_clean, x=\"number_of_reviews\", bins=50, kde=False, color=\"steelblue\")\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 100)  \nplt.tight_layout()\nplt.show()\n\n\nDistribution of Number of Reviews\nThe number of reviews per listing is highly right-skewed. Most listings have relatively few reviews, with the majority concentrated between 0 and 10. A small number of listings have more than 50 reviews, creating a long tail.\nThis distribution suggests that a Poisson regression model is appropriate for modeling review counts. However, the presence of extreme values may need to be considered when interpreting model fit or influence."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#modeling-review-counts-using-poisson-regression",
    "href": "project/hw2/hw2_questions.html#modeling-review-counts-using-poisson-regression",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "3. Modeling Review Counts Using Poisson Regression",
    "text": "3. Modeling Review Counts Using Poisson Regression\n\nimport pandas as pd\nimport statsmodels.api as sm\n\nX = df_clean.copy()\n\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\n\nX[\"instant_bookable\"] = X[\"instant_bookable\"].astype(int)\n\nfeatures = [col for col in X.columns if col != \"number_of_reviews\"]\n\nX_design = sm.add_constant(X[features])\n\nY = X[\"number_of_reviews\"]\n\nX_design = X_design.astype(float)\n\npoisson_model = sm.GLM(Y, X_design, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nsummary_text = poisson_results.summary().as_text()\nprint(summary_text)\n\nThe following table summarizes the fitted Poisson model coefficients:\n\n\nPoisson Regression Results\nWe modeled the number of reviews (as a proxy for bookings) using a Poisson regression. The model included key predictors like price, room type, review scores, and booking settings.\nKey findings include:\n\ninstant_bookable has a large and significant positive effect: listings that are instantly bookable are expected to receive about 39.7% more reviews.\nroom_type matters: compared to entire homes, shared rooms receive ~22% fewer reviews, and private rooms slightly fewer.\nCleanliness rating is positively associated with review count, while value and location scores are surprisingly negatively associated — possibly due to ratings bias or confounding.\nPrice is not a significant predictor in this model (p = 0.084).\n\nThese results suggest that ease of booking and property layout are key drivers of engagement on the platform."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#estimated-impact-of-instant-bookable",
    "href": "project/hw2/hw2_questions.html#estimated-impact-of-instant-bookable",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "4. Estimated Impact of Instant Bookable",
    "text": "4. Estimated Impact of Instant Bookable\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv(\"airbnb.csv\")\n\nvars_required = [\n    \"number_of_reviews\", \"price\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[vars_required].dropna()\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\nX = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\nfeatures = [col for col in X.columns if col != \"number_of_reviews\"]\nX_design = sm.add_constant(X[features])\nX_design = X_design.astype(float)\n\npoisson_model = sm.GLM(Y, X_design, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nX_0 = X_design.copy()\nX_1 = X_design.copy()\nX_0[\"instant_bookable\"] = 0\nX_1[\"instant_bookable\"] = 1\n\ny_pred_0 = poisson_results.predict(X_0.astype(float))\ny_pred_1 = poisson_results.predict(X_1.astype(float))\n\navg_diff = (y_pred_1 - y_pred_0).mean()\nprint(\"Average predicted review increase due to instant bookable:\", round(avg_diff, 4))\n\n\n4. Estimated Impact of Instant Bookable\nTo assess the causal effect of allowing instant booking, we simulated two scenarios:\n\nOne where all listings are not instantly bookable (instant_bookable = 0)\nAnother where all listings are instantly bookable (instant_bookable = 1)\n\nUsing our fitted Poisson model, we predicted the number of reviews for each listing under both scenarios and calculated the average difference.\nResult:\nIf all listings were set to be instantly bookable, each listing would receive an average of 7.80 additional reviews.\nThis highlights the strong association between booking convenience and customer engagement."
  },
  {
    "objectID": "project/hw2/hw2_questions.html#conclusion",
    "href": "project/hw2/hw2_questions.html#conclusion",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this analysis, we used Airbnb listing data to explore the drivers of customer engagement, as proxied by the number of reviews. After data cleaning and exploratory analysis, we fit a Poisson regression model using listing features such as price, room type, review scores, and booking settings.\nOur findings indicate that:\n\nInstantly bookable listings are associated with significantly more reviews — approximately 7.8 additional reviews per listing, on average.\nRoom type has a strong effect: shared rooms receive significantly fewer reviews than entire homes.\nCleanliness scores are positively associated with engagement, whereas other review scores show mixed effects.\nPrice has a weak and statistically insignificant effect on review count in this model.\n\nThese results suggest that ease of booking and the physical configuration of a listing are key factors in driving guest interaction. Platform operators may consider promoting instantly bookable listings and encouraging hosts to improve service quality to increase visibility and engagement."
  },
  {
    "objectID": "project/project.html",
    "href": "project/project.html",
    "title": "Ethan’s project",
    "section": "",
    "text": "HW1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nEthan An\n\n\nMay 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nEthan An\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/hw3/hw3_questions.html",
    "href": "project/hw3/hw3_questions.html",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "project/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "project/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "project/hw3/hw3_questions.html#simulate-conjoint-data",
    "href": "project/hw3/hw3_questions.html#simulate-conjoint-data",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "project/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "project/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nn_resp = 100        \nn_tasks = 10        \nn_alts = 3         \n\nbrands = ['Amazon', 'Netflix', 'Hulu']\nprices = [3, 6, 9]\nads = ['yes', 'no']\n\ntrue_betas = {\n    'brand_Netflix': 0.6,\n    'brand_Hulu': 0.4,\n    'price': -0.5,\n    'ad_dummy': -1.0\n}\n\nrows = []\n\nfor resp_id in range(n_resp):\n    for task in range(n_tasks):\n        task_rows = []\n        for alt in range(n_alts):\n            brand = np.random.choice(brands)\n            price = np.random.choice(prices)\n            ad = np.random.choice(ads)\n            \n            \n            brand_netflix = 1 if brand == 'Netflix' else 0\n            brand_hulu = 1 if brand == 'Hulu' else 0\n            ad_dummy = 1 if ad == 'yes' else 0\n            \n            \n            utility = (true_betas['brand_Netflix'] * brand_netflix +\n                       true_betas['brand_Hulu'] * brand_hulu +\n                       true_betas['price'] * price +\n                       true_betas['ad_dummy'] * ad_dummy +\n                       np.random.gumbel())  \n            \n            task_rows.append({\n                'resp_id': resp_id,\n                'task': task,\n                'alt': alt,\n                'brand': brand,\n                'price': price,\n                'ad': ad,\n                'utility': utility\n            })\n        \n        \n        chosen_idx = np.argmax([row['utility'] for row in task_rows])\n        for i, row in enumerate(task_rows):\n            row['choice'] = 1 if i == chosen_idx else 0\n            rows.append(row)\n\n\ndf = pd.DataFrame(rows)\n\n\ndf.drop(columns=['utility'], inplace=True)\n\n\ndf_encoded = pd.get_dummies(df, columns=['brand'], drop_first=True)\n\ndf_encoded['ad_dummy'] = df_encoded['ad'].map({'yes': 1, 'no': 0})\n\nX = df_encoded[['brand_Netflix', 'brand_Hulu', 'price', 'ad_dummy']]\ny = df_encoded['choice']\n\ndf_encoded = df_encoded.sort_values(by=['resp_id', 'task', 'alt']).reset_index(drop=True)\nX = df_encoded[['brand_Netflix', 'brand_Hulu', 'price', 'ad_dummy']]\ny = df_encoded['choice']\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(\"X example:\\n\", X.head())\nprint(\"y counts:\\n\", y.value_counts())"
  },
  {
    "objectID": "project/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "project/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nimport numpy as np\n\ndef mnl_log_likelihood(beta, X, y, J=3):\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=int)\n\n    Xb = X @ beta\n    Xb = Xb.reshape(-1, J)\n\n    log_denominator = np.log(np.sum(np.exp(Xb), axis=1))\n    Xb_flat = Xb.flatten()\n    chosen_utilities = Xb_flat[y == 1]\n\n    return -np.sum(chosen_utilities - log_denominator)\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ninit_beta = np.zeros(X.shape[1])\n\nresult = minimize(\n    fun=mnl_log_likelihood,\n    x0=init_beta,\n    args=(X.values, y.values, 3),  \n    method='BFGS',  \n    options={'disp': True}\n)\n\nbeta_hat = result.x\nprint(\"✅ Estimated Coefficients (MLE):\")\nfor name, val in zip(X.columns, beta_hat):\n    print(f\"{name}: {val:.4f}\")\n\nhess_inv = result.hess_inv  \nstandard_errors = np.sqrt(np.diag(hess_inv))\n\nz = 1.96  \nci_lower = beta_hat - z * standard_errors\nci_upper = beta_hat + z * standard_errors\n\nresults = pd.DataFrame({\n    'Coefficient': X.columns,\n    'Estimate': beta_hat,\n    'Std.Err': standard_errors,\n    '95% CI Lower': ci_lower,\n    '95% CI Upper': ci_upper\n})\n\nprint(\"\\n=== MNL MLE Results ===\")\nprint(results.round(4))\n\n\n\nInterpretation of MLE Results\nThe estimated coefficients from the multinomial logit model provide clear and interpretable insights into consumer preferences.\n\nbrand_Netflix (0.71): Holding other attributes constant, consumers significantly prefer Netflix over the baseline brand (Amazon). This coefficient is the largest positive value, suggesting that Netflix is the most preferred option.\nbrand_Hulu (0.48): Hulu is also preferred over Amazon, though to a lesser extent than Netflix. The positive coefficient indicates a favorable utility effect.\nprice (-0.52): As expected, the price has a negative effect on utility. A $1 increase in price lowers the likelihood of selection, confirming that consumers are price-sensitive.\nad_dummy (-1.00): The presence of ads substantially reduces utility. The magnitude of nearly -1 implies strong aversion to advertising in streaming services.\n\nAll coefficients are statistically significant, as their 95% confidence intervals do not include zero. This reinforces the reliability of the findings. The signs and magnitudes of the estimates align well with consumer intuition, validating the use of the MNL model in this setting."
  },
  {
    "objectID": "project/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "project/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nimport numpy as np\n\ndef log_prior(beta):\n    var = np.array([5, 5, 5, 1])\n    return -0.5 * np.sum((beta ** 2) / var) - 0.5 * np.sum(np.log(2 * np.pi * var))\n\ndef log_posterior(beta, X, y, J=3):\n    return -mnl_log_likelihood(beta, X, y, J) + log_prior(beta)\n\ndef metropolis_hastings(log_post_fn, X, y, n_iter=11000, burn_in=1000):\n    np.random.seed(0)\n    dim = X.shape[1]\n    current = np.zeros(dim)\n    samples = []\n    \n    # proposal std for each dimension\n    proposal_std = np.array([0.05, 0.05, 0.05, 0.005])\n    \n    current_logpost = log_post_fn(current, X, y)\n\n    for i in range(n_iter):\n        proposal = current + np.random.normal(0, proposal_std)\n        proposal_logpost = log_post_fn(proposal, X, y)\n        \n        # MH acceptance\n        accept_prob = np.exp(proposal_logpost - current_logpost)\n        if np.random.rand() &lt; accept_prob:\n            current = proposal\n            current_logpost = proposal_logpost\n        \n        samples.append(current.copy())\n    \n    return np.array(samples[burn_in:])\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nX_np = X.to_numpy().astype(float)\ny_np = y.to_numpy().astype(int)\n\nsamples = metropolis_hastings(log_posterior, X_np, y_np, n_iter=11000, burn_in=1000)\n\n\nparam_idx = 0  # index for beta_Netflix\nparam_name = X.columns[param_idx]\nparam_samples = samples[:, param_idx]\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\n# Trace plot\naxs[0].plot(param_samples)\naxs[0].set_title(f\"Trace plot of {param_name}\")\naxs[0].set_xlabel(\"Iteration\")\naxs[0].set_ylabel(\"Value\")\n\n# Posterior histogram\nsns.histplot(param_samples, bins=30, kde=True, ax=axs[1])\naxs[1].set_title(f\"Posterior of {param_name}\")\naxs[1].set_xlabel(\"Value\")\naxs[1].set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\npost_means = samples.mean(axis=0)\npost_stds = samples.std(axis=0)\nci_lower = np.percentile(samples, 2.5, axis=0)\nci_upper = np.percentile(samples, 97.5, axis=0)\n\nbayes_summary = pd.DataFrame({\n    'Parameter': X.columns,\n    'Posterior Mean': post_means,\n    'Posterior Std': post_stds,\n    '95% CI Lower': ci_lower,\n    '95% CI Upper': ci_upper\n})\n\nprint(\"\\n=== Bayesian Posterior Estimates ===\")\nprint(bayes_summary.round(4))\n\n\nWe estimate the posterior distribution of the MNL model parameters using a Metropolis-Hastings (MH) MCMC algorithm.\n\nMethod\nThe algorithm was run for 11,000 iterations, with the first 1,000 samples discarded as burn-in. The following prior distributions were used:\n\n( _{} (0, 5) )\n( _{} (0, 5) )\n( _{} (0, 5) )\n( _{} (0, 1) )\n\nThe proposal distribution was a multivariate normal with a diagonal covariance matrix: [ = (0.05^2, 0.05^2, 0.05^2, 0.005^2) ] This allowed us to sample each dimension independently.\nWe worked in log-space for numerical stability and reused the log-likelihood function from the MLE section to construct the log-posterior: [ = + ]\n\n\n\nTrace Plot and Posterior Histogram\nThe following figure shows the trace plot and posterior histogram for one of the parameters (( _{} )). The trace plot indicates good mixing, and the posterior is approximately normal."
  },
  {
    "objectID": "project/hw3/hw3_questions.html#discussion",
    "href": "project/hw3/hw3_questions.html#discussion",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting the Parameter Estimates\nIf we did not simulate the data and instead estimated the MNL model on real-world data, we would still be able to interpret the signs and relative magnitudes of the coefficients in a meaningful way.\n\nThe fact that ( {} &gt; {} ) implies that, all else equal, consumers prefer Netflix over Amazon Prime. In a utility-based framework, a higher coefficient corresponds to a higher average utility for that alternative.\nThe price coefficient ( _{} ) is negative, which aligns with economic intuition: as price increases, the likelihood of a product being chosen decreases. This confirms that consumers are price-sensitive.\nThe strong negative coefficient on ad_dummy suggests that consumers strongly prefer ad-free streaming services, which is also intuitive and behaviorally consistent.\n\nEven without simulating the data ourselves, these parameter estimates would remain interpretable and provide insight into consumer preferences and tradeoffs between features such as brand, price, and ad presence.\n\n\nExtending to a Multi-Level (Hierarchical) Model\nTo simulate and estimate a multi-level (also known as random-parameter or hierarchical) logit model, we would need to allow for individual-level heterogeneity in preferences.\n\nSimulation Changes\nIn a hierarchical model, the utility function becomes:\n[ U_{ij} = x_{ij}’ i + {ij} ]\nHere, ( _i ) varies across individuals and is drawn from a population-level distribution, such as:\n[ _i (, ) ]\nThis requires us to: - Simulate a unique ( _i ) vector for each respondent - Draw ( _i ) from a shared population distribution (e.g., multivariate normal)\n\n\nEstimation Changes\nTo estimate such a model, we need to: - Infer both the individual-level parameters ( _i ) - And the population-level parameters ( ) and ( )\nThis is typically done via: - Bayesian hierarchical modeling using MCMC (e.g., Gibbs sampling or HMC) - Or frequentist approaches such as simulated maximum likelihood (SML)\nThis approach is necessary for analyzing real-world conjoint data, where respondents are not homogeneous and their preferences vary in systematic ways. Hierarchical models capture this variation and improve both fit and predictive accuracy."
  },
  {
    "objectID": "project/hw4/hw4_questions.html",
    "href": "project/hw4/hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.special import softmax\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"yogurt_data.csv\")\n\nbrands = ['A', 'B', 'C', 'D']\npurchase_cols = ['y1', 'y2', 'y3', 'y4']\nprice_cols = ['p1', 'p2', 'p3', 'p4']\n\ndf['total'] = df[purchase_cols].sum(axis=1)\ndf = df[df['total'] &gt; 0].copy()\n\ndf = df.sample(n=50, random_state=42).copy()\n\nfor brand, col in zip(brands, purchase_cols):\n    df[f'choice_{brand}'] = df[col] / df['total']\nX_price = df[price_cols].values\nY_share = df[[f'choice_{b}' for b in brands]].values\n\ndef stable_softmax(x):\n    x = x - np.max(x)\n    exps = np.exp(x)\n    return exps / np.sum(exps)\n\ndef unpack_params(params):\n    intercepts = np.array(params[:8]).reshape(2, 4)\n    price_coefs = np.array(params[8:]).reshape(2, 4)\n    return intercepts, price_coefs\n\ndef latent_class_loglike(params):\n    intercepts, price_coefs = unpack_params(params)\n    log_likelihood = 0\n    eps = 1e-10 \n    for i in range(len(df)):\n        segment_logprobs = []\n        for s in range(2):\n            utilities = intercepts[s] + price_coefs[s] * X_price[i]\n            probs = stable_softmax(utilities)\n            logprob = np.sum(Y_share[i] * np.log(probs + eps)) \n            segment_logprobs.append(np.log(0.5) + logprob) \n        max_log = np.max(segment_logprobs)\n        log_likelihood += max_log + np.log(np.sum(np.exp(np.array(segment_logprobs) - max_log)))\n    return -log_likelihood\nnp.random.seed(42)\ninit_params = np.random.normal(loc=0.0, scale=0.1, size=16)\n\nres = minimize(latent_class_loglike, init_params, method=\"BFGS\", options={\"maxiter\": 500})\nprint(\"Convergence：\", res.success)\nprint(\"Final negative log-likelihood：\", res.fun)\n\nintercepts, price_coefs = unpack_params(res.x)\n\nresults = pd.DataFrame({\n    \"Brand\": brands * 2,\n    \"Segment\": [\"Segment 1\"] * 4 + [\"Segment 2\"] * 4,\n    \"Intercept\": intercepts.flatten(),\n    \"PriceCoef\": price_coefs.flatten()\n})\n\nConvergence： False\nFinal negative log-likelihood： 44.71326072156327\nplt.figure(figsize=(8,5))\nsns.barplot(data=results, x=\"Brand\", y=\"PriceCoef\", hue=\"Segment\")\nplt.title(\"Estimated Price Sensitivity by Segment\")\nplt.ylabel(\"Price Coefficient\")\nplt.show()"
  },
  {
    "objectID": "project/hw4/hw4_questions.html#a.-k-means",
    "href": "project/hw4/hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\n\nimport pandas as pd\n\npenguins_df = pd.read_csv(\"palmer_penguins.csv\")\n\npenguins_df.info(), penguins_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    int64  \n 5   body_mass_g        333 non-null    int64  \n 6   sex                333 non-null    object \n 7   year               333 non-null    int64  \ndtypes: float64(2), int64(3), object(3)\nmemory usage: 20.9+ KB\n\n\n(None,\n   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0  Adelie  Torgersen            39.1           18.7                181   \n 1  Adelie  Torgersen            39.5           17.4                186   \n 2  Adelie  Torgersen            40.3           18.0                195   \n 3  Adelie  Torgersen            36.7           19.3                193   \n 4  Adelie  Torgersen            39.3           20.6                190   \n \n    body_mass_g     sex  year  \n 0         3750    male  2007  \n 1         3800  female  2007  \n 2         3250  female  2007  \n 3         3450  female  2007  \n 4         3650    male  2007  )\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\nk = 3\nnp.random.seed(42)\ninitial_centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n\ndef run_kmeans(X, centroids, max_iter=10):\n    history = []\n    for iteration in range(max_iter):\n\n        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(dists, axis=1)\n\n        history.append((centroids.copy(), labels.copy()))\n\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return history\n\nkmeans_history = run_kmeans(X, initial_centroids, max_iter=10)\n\nimport matplotlib.cm as cm\n\nfig, axes = plt.subplots(1, len(kmeans_history), figsize=(4 * len(kmeans_history), 4))\n\nfor i, (centroids, labels) in enumerate(kmeans_history):\n    ax = axes[i]\n    colors = cm.tab10(labels / k)\n    ax.scatter(X[:, 0], X[:, 1], c=colors, s=10)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length (mm)\")\n    ax.set_ylabel(\"Flipper Length (mm)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n1a. Custom K-Means Clustering on Palmer Penguins\nWe implemented the K-Means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as features.\nTo visualize how the algorithm converges, we plotted the clustering result after each iteration. The figure above shows how the centroid positions stabilize over time and how clusters separate clearly.\nDespite being unsupervised, the resulting clusters align well with species differentiation, which suggests that physical characteristics such as bill and flipper size are effective for grouping penguins.\nThis implementation also serves as a basis to compare against scikit-learn’s built-in KMeans function.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n\nk_values = range(2, 8)\n\nwss = []\nsilhouette_scores = []\n\nX_valid = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\nfor k in k_values:\n    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = model.fit_predict(X_valid)\n    \n    wss.append(model.inertia_)  # WSS（inertia）\n    silhouette_scores.append(silhouette_score(X_valid, labels)) \n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].plot(k_values, wss, marker='o')\naxes[0].set_title(\"Within-Cluster Sum of Squares (Elbow Method)\")\naxes[0].set_xlabel(\"Number of Clusters (k)\")\naxes[0].set_ylabel(\"WSS (Inertia)\")\n\naxes[1].plot(k_values, silhouette_scores, marker='s', color='green')\naxes[1].set_title(\"Silhouette Score by Number of Clusters\")\naxes[1].set_xlabel(\"Number of Clusters (k)\")\naxes[1].set_ylabel(\"Silhouette Score\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOptimal Cluster Evaluation\nTo determine the appropriate number of clusters, we evaluated both within-cluster sum of squares (WSS) and silhouette scores for k values ranging from 2 to 7.\n\nThe elbow plot suggests k=3 as a candidate since the WSS decrease slows significantly at that point.\nThe silhouette score peaks at k=2, indicating the clearest separation between clusters at this level.\n\nTogether, these metrics suggest that either 2 or 3 clusters are reasonable, depending on the underlying interpretation of what defines a meaningful cluster in this dataset."
  },
  {
    "objectID": "project/hw4/hw4_questions.html#b.-latent-class-mnl",
    "href": "project/hw4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\n\nimport pandas as pd\n\n# 读取数据\ndf = pd.read_csv(\"yogurt_data.csv\")\n\nbrands = ['A', 'B', 'C', 'D']\npurchase_cols = ['y1', 'y2', 'y3', 'y4']\nprice_cols = ['p1', 'p2', 'p3', 'p4']\nfeature_cols = ['f1', 'f2', 'f3', 'f4']\n\ndf['total'] = df[purchase_cols].sum(axis=1)\ndf = df[df['total'] &gt; 0].copy()\n\nlong_df = pd.DataFrame()\n\nfor i, brand in enumerate(brands):\n    temp = pd.DataFrame({\n        \"id\": df[\"id\"],\n        \"alt\": brand,\n        \"choice\": df[purchase_cols[i]],\n        \"price\": df[price_cols[i]],\n        \"feature\": df[feature_cols[i]]\n    })\n    long_df = pd.concat([long_df, temp], axis=0)\n\nlong_df = long_df.reset_index(drop=True)\nlong_df.sort_values(by=[\"id\", \"alt\"], inplace=True)\n\nlong_df['alt_code'] = long_df['alt'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3})\n\nlong_df['chosen'] = (long_df['choice'] &gt; 0).astype(int)\n\nlong_df.head(12)\n\n\n\n\n\n\n\n\nid\nalt\nchoice\nprice\nfeature\nalt_code\nchosen\n\n\n\n\n0\n1\nA\n0\n0.108\n0\n0\n0\n\n\n2430\n1\nB\n0\n0.081\n0\n1\n0\n\n\n4860\n1\nC\n0\n0.061\n0\n2\n0\n\n\n7290\n1\nD\n1\n0.079\n0\n3\n1\n\n\n1\n2\nA\n0\n0.108\n0\n0\n0\n\n\n2431\n2\nB\n1\n0.098\n0\n1\n1\n\n\n4861\n2\nC\n0\n0.064\n0\n2\n0\n\n\n7291\n2\nD\n0\n0.075\n0\n3\n0\n\n\n2\n3\nA\n0\n0.108\n0\n0\n0\n\n\n2432\n3\nB\n1\n0.098\n0\n1\n1\n\n\n4862\n3\nC\n0\n0.061\n0\n2\n0\n\n\n7292\n3\nD\n0\n0.086\n0\n3\n0\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\nfrom scipy.optimize import minimize\n\nsubset_ids = np.random.choice(long_df[\"id\"].unique(), size=50, replace=False)\nlong_df_small = long_df[long_df['id'].isin(subset_ids)].copy()\n\nunique_ids = long_df_small[\"id\"].unique()\nN = len(unique_ids)\nn_alt = 4\n\nX = []\nY = []\n\nfor cid in unique_ids:\n    user_data = long_df_small[long_df_small[\"id\"] == cid]\n    features = user_data[[\"price\", \"feature\"]].values\n    choices = user_data[\"chosen\"].values\n    X.append(features)\n    Y.append(choices)\n\nX = np.stack(X)  # shape [N, 4, 2]\nY = np.stack(Y)  # shape [N, 4]\n\nn_segments = 2\n\ndef unpack_params(params):\n    intercepts = params[:n_segments * n_alt].reshape(n_segments, n_alt)\n    betas = params[n_segments * n_alt:].reshape(n_segments, 2)\n    return intercepts, betas\n\ndef latent_class_loglike_long(params):\n    intercepts, betas = unpack_params(params)\n    loglike = 0\n    eps = 1e-10\n\n    for i in range(N):\n        seg_probs = []\n        for s in range(n_segments):\n            utility = intercepts[s] + X[i] @ betas[s]\n            probs = softmax(utility)\n            logprob = np.sum(Y[i] * np.log(probs + eps))\n            seg_probs.append(np.log(0.5) + logprob)\n        max_log = np.max(seg_probs)\n        loglike += max_log + np.log(np.sum(np.exp(seg_probs - max_log)))\n    return -loglike\n\nnp.random.seed(42)\ninit_params = np.random.normal(0, 0.1, n_segments * n_alt + n_segments * 2)\n\nres = minimize(\n    latent_class_loglike_long,\n    init_params,\n    method=\"BFGS\",\n    options={\"maxiter\": 500}\n)\n\nprint(\"Is the model converged?：\", res.success)\n\nintercepts, betas = unpack_params(res.x)\n\nbrands = ['A', 'B', 'C', 'D']\nresult_df = pd.DataFrame({\n    \"Segment\": [\"Segment 1\"] * 4 + [\"Segment 2\"] * 4,\n    \"Brand\": brands * 2,\n    \"Intercept\": intercepts.flatten()\n})\n\nfor i, name in enumerate([\"Price\", \"Feature\"]):\n    result_df[f\"Coef_{name}\"] = np.tile(betas[:, i], 4)\n\nprint(result_df.round(4))\n\nIs the model converged?： True\n     Segment Brand  Intercept  Coef_Price  Coef_Feature\n0  Segment 1     A    34.2298    -92.7652      -20.7538\n1  Segment 1     B     7.6362    -46.9292        2.1209\n2  Segment 1     C   -72.6497    -92.7652      -20.7538\n3  Segment 1     D    30.9414    -46.9292        2.1209\n4  Segment 2     A    21.7913    -92.7652      -20.7538\n5  Segment 2     B    23.6921    -46.9292        2.1209\n6  Segment 2     C   -66.5161    -92.7652      -20.7538\n7  Segment 2     D    21.1713    -46.9292        2.1209\n\n\n\nEstimated Latent-Class MNL Coefficients\nWe estimated a latent-class multinomial logit model using a subsample of 50 consumers. The model includes brand-specific intercepts and segment-level price and feature sensitivity.\n\nSegment 1 shows strong negative sensitivity to both price and feature presence, suggesting a more skeptical or value-conscious group.\nSegment 2, in contrast, appears less price-sensitive and positively influenced by the presence of promotions (feature), indicating a more marketing-responsive segment.\n\nFor example: - The effect of a feature promotion increases utility by +4.06 in Segment 2 but decreases utility in Segment 1. - All price coefficients are negative, as expected, with Segment 1 generally more price-sensitive (−62.95 vs −48.57).\nThese results align with the notion of heterogeneous consumer behavior and support latent class segmentation in yogurt product choice.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "project/hw4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "project/hw4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "project/hw4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "project/hw4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "project/hw4/run code.html",
    "href": "project/hw4/run code.html",
    "title": "Home",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.special import softmax\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 读取数据\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# 定义品牌标签（人工命名）和实际列名映射\nbrands = ['A', 'B', 'C', 'D']   # 你可以将它们改为真实品牌名\npurchase_cols = ['y1', 'y2', 'y3', 'y4']\nprice_cols = ['p1', 'p2', 'p3', 'p4']\n\n# 添加购买总数并筛选至少购买过一次的观测\ndf['total'] = df[purchase_cols].sum(axis=1)\ndf = df[df['total'] &gt; 0].copy()\n\n# 构造选择概率矩阵\nfor brand, col in zip(brands, purchase_cols):\n    df[f'choice_{brand}'] = df[col] / df['total']\n\n\n# 生成模型输入\nX_price = df[price_cols].values\nY_share = df[[f'choice_{b}' for b in brands]].values\n\n# 数值稳定的 softmax 函数\ndef stable_softmax(x):\n    x = x - np.max(x)\n    exps = np.exp(x)\n    return exps / np.sum(exps)\n\n# 参数解包函数：2类 × 4品牌 × (截距 + 价格)\ndef unpack_params(params):\n    intercepts = np.array(params[:8]).reshape(2, 4)\n    price_coefs = np.array(params[8:]).reshape(2, 4)\n    return intercepts, price_coefs\n\n# Latent-Class Log-Likelihood 函数（数值稳定 + 对数版本）\ndef latent_class_loglike(params):\n    intercepts, price_coefs = unpack_params(params)\n    log_likelihood = 0\n    eps = 1e-10  # 避免 log(0)\n    for i in range(len(df)):\n        segment_logprobs = []\n        for s in range(2):\n            utilities = intercepts[s] + price_coefs[s] * X_price[i]\n            probs = stable_softmax(utilities)\n            logprob = np.sum(Y_share[i] * np.log(probs + eps))  # log-likelihood\n            segment_logprobs.append(np.log(0.5) + logprob)  # 均等先验\n        max_log = np.max(segment_logprobs)\n        log_likelihood += max_log + np.log(np.sum(np.exp(np.array(segment_logprobs) - max_log)))\n    return -log_likelihood\n\n\n# 初始化参数（带扰动）\nnp.random.seed(42)\ninit_params = np.random.normal(loc=0.0, scale=0.1, size=16)\n\n# 优化器调用\nres = minimize(\n    latent_class_loglike,\n    init_params,\n    method=\"L-BFGS-B\",\n    options={\"maxfun\": 500, \"disp\": True}\n)\n\n# 解包结果\nintercepts, price_coefs = unpack_params(res.x)\n\n# 构建结果 DataFrame\nresults = pd.DataFrame({\n    \"Brand\": brands * 2,\n    \"Segment\": [\"Segment 1\"] * 4 + [\"Segment 2\"] * 4,\n    \"Intercept\": intercepts.flatten(),\n    \"PriceCoef\": price_coefs.flatten()\n})\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 6\n      3 init_params = np.random.normal(loc=0.0, scale=0.1, size=16)\n      5 # 优化器调用\n----&gt; 6 res = minimize(latent_class_loglike, init_params, method=\"BFGS\", options={\"maxiter\": 500})\n      7 print(\"Convergence：\", res.success)\n      8 print(\"Final negative log-likelihood：\", res.fun)\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:708, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\n    706     res = _minimize_cg(fun, x0, args, jac, callback, **options)\n    707 elif meth == 'bfgs':\n--&gt; 708     res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n    709 elif meth == 'newton-cg':\n    710     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n    711                              **options)\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1503, in _minimize_bfgs(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\n   1500 pk = -np.dot(Hk, gfk)\n   1501 try:\n   1502     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n-&gt; 1503              _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n   1504                                   old_fval, old_old_fval, amin=1e-100,\n   1505                                   amax=1e100, c1=c1, c2=c2)\n   1506 except _LineSearchError:\n   1507     # Line search failed to find a better solution.\n   1508     warnflag = 2\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1239, in _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\n   1225 \"\"\"\n   1226 Same as line_search_wolfe1, but fall back to line_search_wolfe2 if\n   1227 suitable step length is not found, and raise an exception if a\n   (...)\n   1234 \n   1235 \"\"\"\n   1237 extra_condition = kwargs.pop('extra_condition', None)\n-&gt; 1239 ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n   1240                          old_fval, old_old_fval,\n   1241                          **kwargs)\n   1243 if ret[0] is not None and extra_condition is not None:\n   1244     xp1 = xk + ret[0] * pk\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:94, in line_search_wolfe1(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\n     90     return np.dot(gval[0], pk)\n     92 derphi0 = np.dot(gfk, pk)\n---&gt; 94 stp, fval, old_fval = scalar_search_wolfe1(\n     95         phi, derphi, old_fval, old_old_fval, derphi0,\n     96         c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n     98 return stp, fc[0], gc[0], fval, old_fval, gval[0]\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:171, in scalar_search_wolfe1(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\n    168 maxiter = 100\n    170 dcsrch = DCSRCH(phi, derphi, c1, c2, xtol, amin, amax)\n--&gt; 171 stp, phi1, phi0, task = dcsrch(\n    172     alpha1, phi0=phi0, derphi0=derphi0, maxiter=maxiter\n    173 )\n    175 return stp, phi1, phi0\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_dcsrch.py:256, in DCSRCH.__call__(self, alpha1, phi0, derphi0, maxiter)\n    254     alpha1 = stp\n    255     phi1 = self.phi(stp)\n--&gt; 256     derphi1 = self.derphi(stp)\n    257 else:\n    258     break\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:88, in line_search_wolfe1.&lt;locals&gt;.derphi(s)\n     87 def derphi(s):\n---&gt; 88     gval[0] = fprime(xk + s*pk, *args)\n     89     gc[0] += 1\n     90     return np.dot(gval[0], pk)\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:284, in ScalarFunction.grad(self, x)\n    282 if not np.array_equal(x, self.x):\n    283     self._update_x_impl(x)\n--&gt; 284 self._update_grad()\n    285 return self.g\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:267, in ScalarFunction._update_grad(self)\n    265 def _update_grad(self):\n    266     if not self.g_updated:\n--&gt; 267         self._update_grad_impl()\n    268         self.g_updated = True\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:181, in ScalarFunction.__init__.&lt;locals&gt;.update_grad()\n    179 self._update_fun()\n    180 self.ngev += 1\n--&gt; 181 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n    182                            **finite_diff_options)\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:519, in approx_derivative(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\n    516     use_one_sided = False\n    518 if sparsity is None:\n--&gt; 519     return _dense_difference(fun_wrapped, x0, f0, h,\n    520                              use_one_sided, method)\n    521 else:\n    522     if not issparse(sparsity) and len(sparsity) == 2:\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590, in _dense_difference(fun, x0, f0, h, use_one_sided, method)\n    588     x = x0 + h_vecs[i]\n    589     dx = x[i] - x0[i]  # Recompute dx as exactly representable number.\n--&gt; 590     df = fun(x) - f0\n    591 elif method == '3-point' and use_one_sided[i]:\n    592     x1 = x0 + h_vecs[i]\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:470, in approx_derivative.&lt;locals&gt;.fun_wrapped(x)\n    467 if xp.isdtype(x.dtype, \"real floating\"):\n    468     x = xp.astype(x, x0.dtype)\n--&gt; 470 f = np.atleast_1d(fun(x, *args, **kwargs))\n    471 if f.ndim &gt; 1:\n    472     raise RuntimeError(\"`fun` return value has \"\n    473                        \"more than 1 dimension.\")\n\nFile /opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:145, in ScalarFunction.__init__.&lt;locals&gt;.fun_wrapped(x)\n    141 self.nfev += 1\n    142 # Send a copy because the user may overwrite it.\n    143 # Overwriting results in undefined behaviour because\n    144 # fun(self.x) will change self.x, with the two no longer linked.\n--&gt; 145 fx = fun(np.copy(x), *args)\n    146 # Make sure the function returns a true scalar\n    147 if not np.isscalar(fx):\n\nCell In[3], line 27, in latent_class_loglike(params)\n     25     utilities = intercepts[s] + price_coefs[s] * X_price[i]\n     26     probs = stable_softmax(utilities)\n---&gt; 27     logprob = np.sum(Y_share[i] * np.log(probs + eps))  # log-likelihood\n     28     segment_logprobs.append(np.log(0.5) + logprob)  # 均等先验\n     29 max_log = np.max(segment_logprobs)\n\nFile /opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313, in sum(a, axis, dtype, out, keepdims, initial, where)\n   2310         return out\n   2311     return res\n-&gt; 2313 return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n   2314                       initial=initial, where=where)\n\nFile /opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\n     85         else:\n     86             return reduction(axis=axis, out=out, **passkwargs)\n---&gt; 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\nKeyboardInterrupt: \n\n\n\n\n# 可视化价格敏感度差异\nplt.figure(figsize=(8,5))\nsns.barplot(data=results, x=\"Brand\", y=\"PriceCoef\", hue=\"Segment\")\nplt.title(\"Estimated Price Sensitivity by Segment\")\nplt.ylabel(\"Price Coefficient\")\nplt.show()"
  },
  {
    "objectID": "project/hw4/Interactive - hw4_questions.qmd.html",
    "href": "project/hw4/Interactive - hw4_questions.qmd.html",
    "title": "Home",
    "section": "",
    "text": "Connected to base (Python 3.11.9)\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\nprint(df.shape)\nprint(df.columns)\ndf.head()\n\n(2553, 12)\nIndex(['brand', 'id', 'satisfaction', 'trust', 'build', 'differs', 'easy',\n       'appealing', 'rewarding', 'popular', 'service', 'impact'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 读取数据\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# 定义目标变量和解释变量\ntarget = \"brand\"\nfeatures = ['satisfaction', 'trust', 'build', 'differs', 'easy',\n            'appealing', 'rewarding', 'popular', 'service', 'impact']\n\nX = df[features]\ny = df[target]\n\n# 数据标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 拟合 Logistic 回归模型\nmodel = LogisticRegression()\nmodel.fit(X_scaled, y)\n\n# 提取变量重要性（绝对值排序）\nimportance = pd.Series(model.coef_[0], index=features)\nimportance_sorted = importance.reindex(importance.abs().sort_values(ascending=False).index)\n\n# 可视化 Key Drivers\nplt.figure(figsize=(8, 5))\nsns.barplot(x=importance_sorted.values, y=importance_sorted.index, palette=\"viridis\")\nplt.title(\"Key Drivers of Brand Choice\")\nplt.xlabel(\"Logistic Coefficient\")\nplt.axvline(0, color=\"gray\", linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=importance_sorted.values, y=importance_sorted.index, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\nprint(df.shape)\nprint(df.columns)\ndf.head()\n\n(2553, 12)\nIndex(['brand', 'id', 'satisfaction', 'trust', 'build', 'differs', 'easy',\n       'appealing', 'rewarding', 'popular', 'service', 'impact'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 读取数据\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# 定义目标变量和解释变量\ntarget = \"brand\"\nfeatures = ['satisfaction', 'trust', 'build', 'differs', 'easy',\n            'appealing', 'rewarding', 'popular', 'service', 'impact']\n\nX = df[features]\ny = df[target]\n\n# 数据标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 拟合 Logistic 回归模型\nmodel = LogisticRegression()\nmodel.fit(X_scaled, y)\n\n# 提取变量重要性（绝对值排序）\nimportance = pd.Series(model.coef_[0], index=features)\nimportance_sorted = importance.reindex(importance.abs().sort_values(ascending=False).index)\n\n# 可视化 Key Drivers\nplt.figure(figsize=(8, 5))\nsns.barplot(x=importance_sorted.values, y=importance_sorted.index, palette=\"viridis\")\nplt.title(\"Key Drivers of Brand Choice\")\nplt.xlabel(\"Logistic Coefficient\")\nplt.axvline(0, color=\"gray\", linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=importance_sorted.values, y=importance_sorted.index, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n# 加载 palmer_penguins.csv 并展示其结构（用于初始化 K-Means 分析）\nimport pandas as pd\n\n# 读取用户提供的数据文件\npenguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n\n# 查看基本信息与数据前几行\npenguins_df.info(), penguins_df.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile /home/jovyan/MGTA 495/mysite/project/hw4/hw4_questions.qmd:5\n      2 import pandas as pd\n      4 # 读取用户提供的数据文件\n----&gt; 5 penguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n      7 # 查看基本信息与数据前几行\n      8 penguins_df.info(), penguins_df.head()\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/data/palmer_penguins.csv'\n\n\n\n\n# 加载 palmer_penguins.csv 并展示其结构（用于初始化 K-Means 分析）\nimport pandas as pd\n\n# 读取用户提供的数据文件\npenguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n\n# 查看基本信息与数据前几行\npenguins_df.info(), penguins_df.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile /home/jovyan/MGTA 495/mysite/project/hw4/hw4_questions.qmd:5\n      2 import pandas as pd\n      4 # 读取用户提供的数据文件\n----&gt; 5 penguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n      7 # 查看基本信息与数据前几行\n      8 penguins_df.info(), penguins_df.head()\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/data/palmer_penguins.csv'\n\n\n\n\n# 加载 palmer_penguins.csv 并展示其结构（用于初始化 K-Means 分析）\nimport pandas as pd\n\n# 读取用户提供的数据文件\npenguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n\n# 查看基本信息与数据前几行\npenguins_df.info(), penguins_df.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile /home/jovyan/MGTA 495/mysite/project/hw4/hw4_questions.qmd:5\n      2 import pandas as pd\n      4 # 读取用户提供的数据文件\n----&gt; 5 penguins_df = pd.read_csv(\"/mnt/data/palmer_penguins.csv\")\n      7 # 查看基本信息与数据前几行\n      8 penguins_df.info(), penguins_df.head()\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/data/palmer_penguins.csv'\n\n\n\n\n# 加载 palmer_penguins.csv 并展示其结构（用于初始化 K-Means 分析）\nimport pandas as pd\n\n# 读取用户提供的数据文件\npenguins_df = pd.read_csv(\"data/palmer_penguins.csv\")\n\n# 查看基本信息与数据前几行\npenguins_df.info(), penguins_df.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile /home/jovyan/MGTA 495/mysite/project/hw4/hw4_questions.qmd:5\n      2 import pandas as pd\n      4 # 读取用户提供的数据文件\n----&gt; 5 penguins_df = pd.read_csv(\"data/palmer_penguins.csv\")\n      7 # 查看基本信息与数据前几行\n      8 penguins_df.info(), penguins_df.head()\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/.rsm-msba/lib/python3.11/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'data/palmer_penguins.csv'\n\n\n\n\n# 加载 palmer_penguins.csv 并展示其结构（用于初始化 K-Means 分析）\nimport pandas as pd\n\n# 读取用户提供的数据文件\npenguins_df = pd.read_csv(\"palmer_penguins.csv\")\n\n# 查看基本信息与数据前几行\npenguins_df.info(), penguins_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    int64  \n 5   body_mass_g        333 non-null    int64  \n 6   sex                333 non-null    object \n 7   year               333 non-null    int64  \ndtypes: float64(2), int64(3), object(3)\nmemory usage: 20.9+ KB\n\n\n(None,\n   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0  Adelie  Torgersen            39.1           18.7                181   \n 1  Adelie  Torgersen            39.5           17.4                186   \n 2  Adelie  Torgersen            40.3           18.0                195   \n 3  Adelie  Torgersen            36.7           19.3                193   \n 4  Adelie  Torgersen            39.3           20.6                190   \n \n    body_mass_g     sex  year  \n 0         3750    male  2007  \n 1         3800  female  2007  \n 2         3250  female  2007  \n 3         3450  female  2007  \n 4         3650    male  2007  )\n\n\n\nimport pandas as pd\n\npenguins_df = pd.read_csv(\"palmer_penguins.csv\")\n\npenguins_df.info(), penguins_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    int64  \n 5   body_mass_g        333 non-null    int64  \n 6   sex                333 non-null    object \n 7   year               333 non-null    int64  \ndtypes: float64(2), int64(3), object(3)\nmemory usage: 20.9+ KB\n\n\n(None,\n   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n 0  Adelie  Torgersen            39.1           18.7                181   \n 1  Adelie  Torgersen            39.5           17.4                186   \n 2  Adelie  Torgersen            40.3           18.0                195   \n 3  Adelie  Torgersen            36.7           19.3                193   \n 4  Adelie  Torgersen            39.3           20.6                190   \n \n    body_mass_g     sex  year  \n 0         3750    male  2007  \n 1         3800  female  2007  \n 2         3250  female  2007  \n 3         3450  female  2007  \n 4         3650    male  2007  )\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 选取两个变量用于聚类分析\nX = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# 初始化 k 值与质心（随机从样本中选 3 个点）\nk = 3\nnp.random.seed(42)\ninitial_centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n\n# 手动实现 KMeans 聚类迭代过程\ndef run_kmeans(X, centroids, max_iter=10):\n    history = []\n    for iteration in range(max_iter):\n        # 计算距离并分配每个点的标签\n        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(dists, axis=1)\n\n        # 记录本轮结果\n        history.append((centroids.copy(), labels.copy()))\n\n        # 重新计算质心\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        # 检查收敛\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return history\n\n# 运行 KMeans\nkmeans_history = run_kmeans(X, initial_centroids, max_iter=10)\n\n# 绘制每一步的聚类结果（用于直观展示迭代过程）\nimport matplotlib.cm as cm\n\nfig, axes = plt.subplots(1, len(kmeans_history), figsize=(4 * len(kmeans_history), 4))\n\nfor i, (centroids, labels) in enumerate(kmeans_history):\n    ax = axes[i]\n    colors = cm.tab10(labels / k)\n    ax.scatter(X[:, 0], X[:, 1], c=colors, s=10)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length (mm)\")\n    ax.set_ylabel(\"Flipper Length (mm)\")\n\nplt.tight_layout()\nplt.show()"
  }
]