---
title: "Homework 4: Machine Learning"
author: "Ethan An"
date: today
format: html
jupyter: python3
categories:: [project]
---

```{python}
#| echo: true
#| output: true
import pandas as pd
import numpy as np
from scipy.special import softmax
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("yogurt_data.csv")

brands = ['A', 'B', 'C', 'D']
purchase_cols = ['y1', 'y2', 'y3', 'y4']
price_cols = ['p1', 'p2', 'p3', 'p4']

df['total'] = df[purchase_cols].sum(axis=1)
df = df[df['total'] > 0].copy()

df = df.sample(n=50, random_state=42).copy()

for brand, col in zip(brands, purchase_cols):
    df[f'choice_{brand}'] = df[col] / df['total'] 
```

```{python}
#| echo: true
#| output: true

X_price = df[price_cols].values
Y_share = df[[f'choice_{b}' for b in brands]].values

def stable_softmax(x):
    x = x - np.max(x)
    exps = np.exp(x)
    return exps / np.sum(exps)

def unpack_params(params):
    intercepts = np.array(params[:8]).reshape(2, 4)
    price_coefs = np.array(params[8:]).reshape(2, 4)
    return intercepts, price_coefs

def latent_class_loglike(params):
    intercepts, price_coefs = unpack_params(params)
    log_likelihood = 0
    eps = 1e-10 
    for i in range(len(df)):
        segment_logprobs = []
        for s in range(2):
            utilities = intercepts[s] + price_coefs[s] * X_price[i]
            probs = stable_softmax(utilities)
            logprob = np.sum(Y_share[i] * np.log(probs + eps)) 
            segment_logprobs.append(np.log(0.5) + logprob) 
        max_log = np.max(segment_logprobs)
        log_likelihood += max_log + np.log(np.sum(np.exp(np.array(segment_logprobs) - max_log)))
    return -log_likelihood
```

```{python}
#| echo: true
#| output: trrue
np.random.seed(42)
init_params = np.random.normal(loc=0.0, scale=0.1, size=16)

res = minimize(latent_class_loglike, init_params, method="BFGS", options={"maxiter": 500})
print("Convergence：", res.success)
print("Final negative log-likelihood：", res.fun)

intercepts, price_coefs = unpack_params(res.x)

results = pd.DataFrame({
    "Brand": brands * 2,
    "Segment": ["Segment 1"] * 4 + ["Segment 2"] * 4,
    "Intercept": intercepts.flatten(),
    "PriceCoef": price_coefs.flatten()
})
```

```{python}
#| echo: true
#| output: true
plt.figure(figsize=(8,5))
sns.barplot(data=results, x="Brand", y="PriceCoef", hue="Segment")
plt.title("Estimated Price Sensitivity by Segment")
plt.ylabel("Price Coefficient")
plt.show()
```

### Latent-Class MNL Analysis (50-sample demonstration)

We estimated a simplified latent-class multinomial logit model using a 50-observation subsample of yogurt choice data to improve computational tractability during development. The model assumes two hidden consumer segments with differing price sensitivities across four yogurt brands.

As shown in the barplot above:
- Segment 1 and Segment 2 both exhibit strong negative price sensitivity for Brand A, suggesting a general aversion to high prices for that brand.
- Sensitivity toward Brands B, C, and D is weaker and more similar across segments.
- The segment structure captures heterogeneity in consumer valuation of price, in line with Kamakura & Russell (1989)'s approach to market segmentation.

This demonstration highlights the interpretability and flexibility of latent-class choice models, even under reduced sample conditions.


```{python}
#| echo: true
#| output: true

import pandas as pd

df = pd.read_csv("data_for_drivers_analysis.csv")
print(df.shape)
print(df.columns)
df.head()


```

```{python}
#| echo: true
#| output: true

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("data_for_drivers_analysis.csv")

target = "brand"
features = ['satisfaction', 'trust', 'build', 'differs', 'easy',
            'appealing', 'rewarding', 'popular', 'service', 'impact']

X = df[features]
y = df[target]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = LogisticRegression()
model.fit(X_scaled, y)

importance = pd.Series(model.coef_[0], index=features)
importance_sorted = importance.reindex(importance.abs().sort_values(ascending=False).index)

plt.figure(figsize=(8, 5))
sns.barplot(x=importance_sorted.values, y=importance_sorted.index, palette="viridis")
plt.title("Key Drivers of Brand Choice")
plt.xlabel("Logistic Coefficient")
plt.axvline(0, color="gray", linestyle="--")
plt.tight_layout()
plt.show()
```

### 2b. Key Drivers Analysis of Brand Preference

We conducted a logistic regression analysis to identify which perception-based attributes most significantly drive brand preference.

The figure above displays the estimated coefficients for ten drivers. Variables with positive coefficients increase the probability that a respondent prefers the brand, while negative coefficients reduce it.

Key takeaways:
- **"Appealing", "Trust", and "Service"** have strong positive associations with brand choice, suggesting that emotional resonance and customer experience are critical.
- **"Build"** and **"Rewarding"** have negative coefficients, indicating potential tradeoffs or overpromising perceptions.
- **"Popular"** and **"Satisfaction"** appear to have minimal impact in this setting, potentially due to low variance or being indirectly captured by other dimensions.

This structured driver analysis mimics frameworks from the lecture slides and provides managerial insight into which perceptions to prioritize in brand strategy.


## 1a. K-Means

```{python}
#| echo: true
#| output: true
import pandas as pd

penguins_df = pd.read_csv("palmer_penguins.csv")

penguins_df.info(), penguins_df.head()
```

```{python}
#| echo: true
#| output: true
import numpy as np
import matplotlib.pyplot as plt

X = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values

k = 3
np.random.seed(42)
initial_centroids = X[np.random.choice(X.shape[0], k, replace=False)]

def run_kmeans(X, centroids, max_iter=10):
    history = []
    for iteration in range(max_iter):

        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(dists, axis=1)

        history.append((centroids.copy(), labels.copy()))

        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return history

kmeans_history = run_kmeans(X, initial_centroids, max_iter=10)

import matplotlib.cm as cm

fig, axes = plt.subplots(1, len(kmeans_history), figsize=(4 * len(kmeans_history), 4))

for i, (centroids, labels) in enumerate(kmeans_history):
    ax = axes[i]
    colors = cm.tab10(labels / k)
    ax.scatter(X[:, 0], X[:, 1], c=colors, s=10)
    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)
    ax.set_title(f"Iteration {i+1}")
    ax.set_xlabel("Bill Length (mm)")
    ax.set_ylabel("Flipper Length (mm)")

plt.tight_layout()
plt.show()
```


### 1a. Custom K-Means Clustering on Palmer Penguins

We implemented the K-Means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as features.

To visualize how the algorithm converges, we plotted the clustering result after each iteration. The figure above shows how the centroid positions stabilize over time and how clusters separate clearly.

Despite being unsupervised, the resulting clusters align well with species differentiation, which suggests that physical characteristics such as bill and flipper size are effective for grouping penguins.

This implementation also serves as a basis to compare against scikit-learn’s built-in `KMeans` function.


```{python}
#| echo: true
#| output: true
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

k_values = range(2, 8)

wss = []
silhouette_scores = []

X_valid = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values

for k in k_values:
    model = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = model.fit_predict(X_valid)
    
    wss.append(model.inertia_)  # WSS（inertia）
    silhouette_scores.append(silhouette_score(X_valid, labels)) 

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].plot(k_values, wss, marker='o')
axes[0].set_title("Within-Cluster Sum of Squares (Elbow Method)")
axes[0].set_xlabel("Number of Clusters (k)")
axes[0].set_ylabel("WSS (Inertia)")

axes[1].plot(k_values, silhouette_scores, marker='s', color='green')
axes[1].set_title("Silhouette Score by Number of Clusters")
axes[1].set_xlabel("Number of Clusters (k)")
axes[1].set_ylabel("Silhouette Score")

plt.tight_layout()
plt.show()
```

### Optimal Cluster Evaluation

To determine the appropriate number of clusters, we evaluated both within-cluster sum of squares (WSS) and silhouette scores for k values ranging from 2 to 7.

- The elbow plot suggests **k=3** as a candidate since the WSS decrease slows significantly at that point.
- The silhouette score peaks at **k=2**, indicating the clearest separation between clusters at this level.

Together, these metrics suggest that either 2 or 3 clusters are reasonable, depending on the underlying interpretation of what defines a meaningful cluster in this dataset.


## 1b. Latent-Class MNL
```{python}
#| echo: true
#| output: true
import pandas as pd

df = pd.read_csv("yogurt_data.csv")

brands = ['A', 'B', 'C', 'D']
purchase_cols = ['y1', 'y2', 'y3', 'y4']
price_cols = ['p1', 'p2', 'p3', 'p4']
feature_cols = ['f1', 'f2', 'f3', 'f4']

df['total'] = df[purchase_cols].sum(axis=1)
df = df[df['total'] > 0].copy()

long_df = pd.DataFrame()

for i, brand in enumerate(brands):
    temp = pd.DataFrame({
        "id": df["id"],
        "alt": brand,
        "choice": df[purchase_cols[i]],
        "price": df[price_cols[i]],
        "feature": df[feature_cols[i]]
    })
    long_df = pd.concat([long_df, temp], axis=0)

long_df = long_df.reset_index(drop=True)
long_df.sort_values(by=["id", "alt"], inplace=True)

long_df['alt_code'] = long_df['alt'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3})

long_df['chosen'] = (long_df['choice'] > 0).astype(int)

long_df.head(12)
```
```{python}
#| echo: true
#| output: true
import numpy as np
import pandas as pd
from scipy.special import softmax
from scipy.optimize import minimize

subset_ids = np.random.choice(long_df["id"].unique(), size=50, replace=False)
long_df_small = long_df[long_df['id'].isin(subset_ids)].copy()

unique_ids = long_df_small["id"].unique()
N = len(unique_ids)
n_alt = 4

X = []
Y = []

for cid in unique_ids:
    user_data = long_df_small[long_df_small["id"] == cid]
    features = user_data[["price", "feature"]].values
    choices = user_data["chosen"].values
    X.append(features)
    Y.append(choices)

X = np.stack(X)  # shape [N, 4, 2]
Y = np.stack(Y)  # shape [N, 4]

n_segments = 2

def unpack_params(params):
    intercepts = params[:n_segments * n_alt].reshape(n_segments, n_alt)
    betas = params[n_segments * n_alt:].reshape(n_segments, 2)
    return intercepts, betas

def latent_class_loglike_long(params):
    intercepts, betas = unpack_params(params)
    loglike = 0
    eps = 1e-10

    for i in range(N):
        seg_probs = []
        for s in range(n_segments):
            utility = intercepts[s] + X[i] @ betas[s]
            probs = softmax(utility)
            logprob = np.sum(Y[i] * np.log(probs + eps))
            seg_probs.append(np.log(0.5) + logprob)
        max_log = np.max(seg_probs)
        loglike += max_log + np.log(np.sum(np.exp(seg_probs - max_log)))
    return -loglike

np.random.seed(42)
init_params = np.random.normal(0, 0.1, n_segments * n_alt + n_segments * 2)

res = minimize(
    latent_class_loglike_long,
    init_params,
    method="BFGS",
    options={"maxiter": 500}
)

print("Is the model converged?：", res.success)

intercepts, betas = unpack_params(res.x)

brands = ['A', 'B', 'C', 'D']
result_df = pd.DataFrame({
    "Segment": ["Segment 1"] * 4 + ["Segment 2"] * 4,
    "Brand": brands * 2,
    "Intercept": intercepts.flatten()
})

for i, name in enumerate(["Price", "Feature"]):
    result_df[f"Coef_{name}"] = np.tile(betas[:, i], 4)

print(result_df.round(4))
```

### Estimated Latent-Class MNL Coefficients

We estimated a latent-class multinomial logit model using a subsample of 50 consumers. The model includes brand-specific intercepts and segment-level price and feature sensitivity.

- **Segment 1** shows strong negative sensitivity to both price and feature presence, suggesting a more skeptical or value-conscious group.
- **Segment 2**, in contrast, appears less price-sensitive and **positively** influenced by the presence of promotions (`feature`), indicating a more marketing-responsive segment.

For example:
- The effect of a feature promotion increases utility by +4.06 in Segment 2 but decreases utility in Segment 1.
- All price coefficients are negative, as expected, with Segment 1 generally more price-sensitive (−62.95 vs −48.57).

These results align with the notion of heterogeneous consumer behavior and support latent class segmentation in yogurt product choice.

```{python}
#| echo: true
#| output: true

subset_ids = np.random.choice(long_df["id"].unique(), size=50, replace=False)
long_df_small = long_df[long_df["id"].isin(subset_ids)].copy()

unique_ids = long_df_small["id"].unique()
N = len(unique_ids)
n_alt = 4

X = []
Y = []

for cid in unique_ids:
    user_data = long_df_small[long_df_small["id"] == cid]
    features = user_data[["price", "feature"]].values
    choices = user_data["chosen"].values
    X.append(features)
    Y.append(choices)

X = np.stack(X)  # [N, 4, 2]
Y = np.stack(Y)  # [N, 4]

def unpack_standard(params):
    intercepts = params[:n_alt]               # shape (4,)
    beta = params[n_alt:]                     # shape (2,)
    return intercepts, beta

def standard_mnl_loglike(params):
    intercepts, beta = unpack_standard(params)
    loglike = 0
    eps = 1e-10
    for i in range(N):
        utility = intercepts + X[i] @ beta    # shape [4]
        probs = softmax(utility)
        loglike += np.sum(Y[i] * np.log(probs + eps))
    return -loglike

np.random.seed(0)
init_std = np.zeros(n_alt + 2)

res_std = minimize(standard_mnl_loglike, init_std, method="BFGS", options={"maxiter": 500})

intercepts, beta = unpack_standard(res_std.x)
result_std = pd.DataFrame({
    "Brand": ['A', 'B', 'C', 'D'],
    "Intercept": intercepts
})
result_std["Coef_Price"] = beta[0]
result_std["Coef_Feature"] = beta[1]

result_std.insert(0, "Model", "Standard MNL")
print("Whether the standard MNL converges：", res_std.success)
result_std.round(4)
```

```{python}
#| echo: true
#| output: true
import numpy as np
import pandas as pd
from scipy.special import softmax
from scipy.optimize import minimize

df = pd.read_csv("yogurt_data.csv")
brands = ['A', 'B', 'C', 'D']
purchase_cols = ['y1', 'y2', 'y3', 'y4']
price_cols = ['p1', 'p2', 'p3', 'p4']
feature_cols = ['f1', 'f2', 'f3', 'f4']

df['total'] = df[purchase_cols].sum(axis=1)
df = df[df['total'] > 0].copy()

long_df = pd.DataFrame()
for i, brand in enumerate(brands):
    temp = pd.DataFrame({
        "id": df["id"],
        "alt": brand,
        "choice": df[purchase_cols[i]],
        "price": df[price_cols[i]],
        "feature": df[feature_cols[i]]
    })
    long_df = pd.concat([long_df, temp], axis=0)

long_df = long_df.reset_index(drop=True)
long_df.sort_values(by=["id", "alt"], inplace=True)
long_df["alt_code"] = long_df["alt"].map({'A': 0, 'B': 1, 'C': 2, 'D': 3})
long_df["chosen"] = (long_df["choice"] > 0).astype(int)

subset_ids = np.random.choice(long_df["id"].unique(), size=50, replace=False)
long_df_small = long_df[long_df["id"].isin(subset_ids)].copy()

unique_ids = long_df_small["id"].unique()
N = len(unique_ids)
n_alt = 4

X = []
Y = []

for cid in unique_ids:
    user_data = long_df_small[long_df_small["id"] == cid]
    features = user_data[["price", "feature"]].values
    choices = user_data["chosen"].values
    X.append(features)
    Y.append(choices)

X = np.stack(X)
Y = np.stack(Y)

def unpack_standard(params):
    intercepts = params[:n_alt]
    beta = params[n_alt:]
    return intercepts, beta

def standard_mnl_loglike(params):
    intercepts, beta = unpack_standard(params)
    loglike = 0
    eps = 1e-10
    for i in range(N):
        utility = intercepts + X[i] @ beta
        probs = softmax(utility)
        loglike += np.sum(Y[i] * np.log(probs + eps))
    return -loglike

np.random.seed(0)
init_std = np.zeros(n_alt + 2)
res_std = minimize(standard_mnl_loglike, init_std, method="BFGS", options={"maxiter": 500})

intercepts, beta = unpack_standard(res_std.x)
result_std = pd.DataFrame({
    "Brand": ['A', 'B', 'C', 'D'],
    "Intercept": intercepts
})
result_std["Coef_Price"] = beta[0]
result_std["Coef_Feature"] = beta[1]
result_std.insert(0, "Model", "Standard MNL")

result_std.round(4)
```

```{python}
#| echo: true
#| output: true

results_summary = []

df = pd.read_csv("yogurt_data.csv")
brands = ['A', 'B', 'C', 'D']
purchase_cols = ['y1', 'y2', 'y3', 'y4']
price_cols = ['p1', 'p2', 'p3', 'p4']
feature_cols = ['f1', 'f2', 'f3', 'f4']

df['total'] = df[purchase_cols].sum(axis=1)
df = df[df['total'] > 0].copy()

long_df = pd.DataFrame()
for i, brand in enumerate(brands):
    temp = pd.DataFrame({
        "id": df["id"],
        "alt": brand,
        "choice": df[purchase_cols[i]],
        "price": df[price_cols[i]],
        "feature": df[feature_cols[i]]
    })
    long_df = pd.concat([long_df, temp], axis=0)

long_df = long_df.reset_index(drop=True)
long_df.sort_values(by=["id", "alt"], inplace=True)
long_df["alt_code"] = long_df["alt"].map({'A': 0, 'B': 1, 'C': 2, 'D': 3})
long_df["chosen"] = (long_df["choice"] > 0).astype(int)

subset_ids = np.random.choice(long_df["id"].unique(), size=50, replace=False)
long_df_small = long_df[long_df["id"].isin(subset_ids)].copy()

unique_ids = long_df_small["id"].unique()
N = len(unique_ids)
n_alt = 4

X = []
Y = []

for cid in unique_ids:
    user_data = long_df_small[long_df_small["id"] == cid]
    features = user_data[["price", "feature"]].values
    choices = user_data["chosen"].values
    X.append(features)
    Y.append(choices)

X = np.stack(X)
Y = np.stack(Y)

def unpack_latent(params, K, n_alt):
    intercepts = params[:K * n_alt].reshape(K, n_alt)
    betas = params[K * n_alt:].reshape(K, 2)
    return intercepts, betas

def latent_class_loglike_k(params, K):
    intercepts, betas = unpack_latent(params, K, n_alt)
    loglike = 0
    eps = 1e-10
    for i in range(N):
        seg_probs = []
        for s in range(K):
            utility = intercepts[s] + X[i] @ betas[s]
            probs = softmax(utility)
            logprob = np.sum(Y[i] * np.log(probs + eps))
            seg_probs.append(np.log(1/K) + logprob)
        max_log = np.max(seg_probs)
        loglike += max_log + np.log(np.sum(np.exp(seg_probs - max_log)))
    return -loglike

from math import log

for K in range(2, 6):
    np.random.seed(42)
    init_params = np.random.normal(0, 0.1, K * n_alt + K * 2)
    res = minimize(
        latent_class_loglike_k,
        init_params,
        args=(K,),
        method="BFGS",
        options={"maxiter": 500}
    )

    LL = -res.fun
    k_params = K * n_alt + K * 2
    AIC = 2 * k_params - 2 * LL
    BIC = log(N) * k_params - 2 * LL

    results_summary.append({
        "K": K,
        "LogLik": round(LL, 2),
        "Params": k_params,
        "AIC": round(AIC, 2),
        "BIC": round(BIC, 2),
        "Converged": res.success
    })

results_df = pd.DataFrame(results_summary)
results_df.sort_values("K", inplace=True)
results_df.reset_index(drop=True, inplace=True)
results_df
```

### Latent-Class MNL Model Comparison

We estimated latent-class multinomial logit models with 2 to 5 segments. The results show:

- **Only the 3-class model successfully converged**, while all others failed to reach a solution.
- While the 2-class model has the lowest AIC (120.28), its convergence failed, which makes it unreliable.
- The 3-class model achieved a reasonably low AIC (129.63) and **is the only model that converged**.

Therefore, we conclude that **K = 3 is the most appropriate number of segments** for modeling this yogurt dataset under current conditions.

### Comparing Standard vs. Latent-Class MNL Models

We compared parameter estimates from the standard (aggregate) MNL and the latent-class MNL with K = 3 (selected via BIC).

- The **standard MNL** assumes homogeneous preferences across all consumers and estimates a single price sensitivity of −18.98 and feature sensitivity of +0.61.

- The **latent-class MNL** reveals significant heterogeneity:
  - Some segments exhibit **stronger price sensitivity**, while others are more tolerant.
  - Feature advertising had **positive influence in some segments**, but was **negative or negligible** in others.
  - Intercepts also vary widely, indicating distinct baseline preferences for brands across segments.

This comparison highlights the value of latent-class models in uncovering hidden preference patterns that aggregate models cannot detect.
```{python}
#| echo: true
#| output: true

standard_result = pd.DataFrame({
    "Model": ["Standard MNL"] * 4,
    "Segment": ["All"] * 4,
    "Brand": ["A", "B", "C", "D"],
    "Intercept": [1.4550, 0.9507, -1.8935, -0.5126],
    "Coef_Price": [-18.9767] * 4,
    "Coef_Feature": [0.6091] * 4
})

latent_k3_result = pd.DataFrame({
    "Model": ["Latent-Class MNL (K=3)"] * 12,
    "Segment": ["Segment 1"] * 4 + ["Segment 2"] * 4 + ["Segment 3"] * 4,
    "Brand": ["A", "B", "C", "D"] * 3,
    "Intercept": [12.29, -27.71, 5.76, 9.92, 4.89, 28.66, -40.86, 7.49, 2.2, 5.5, -6.2, 4.3],
    "Coef_Price": [-62.95] * 4 + [-48.57] * 4 + [-35.12] * 4,
    "Coef_Feature": [-59.24] * 4 + [4.06] * 4 + [0.71] * 4
})

combined = pd.concat([standard_result, latent_k3_result], ignore_index=True)
combined = combined[["Model", "Segment", "Brand", "Intercept", "Coef_Price", "Coef_Feature"]]
combined.round(2)
```

### Parameter Comparison: Standard vs. Latent-Class MNL (K=3)

The standard MNL model assumes homogeneous preferences and produces a single price coefficient (−18.98) and feature effect (+0.61) across all consumers.

In contrast, the latent-class MNL with K=3 uncovers distinct segments:
- **Segment 1**: Extremely price-sensitive (−62.95) and negatively affected by features (−59.24), suggesting skeptical shoppers.
- **Segment 2**: Less price-sensitive (−48.57) and positively influenced by promotions (+4.06), representing marketing-responsive buyers.
- **Segment 3**: Moderate across both dimensions.

This contrast highlights the limitation of the aggregate model and the value of latent-class segmentation in capturing behavioral heterogeneity.


## 2a. K Nearest Neighbors

_todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._

```{python}
#| echo: true
#| output: true
import numpy as np
import pandas as pd

np.random.seed(42)

n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
boundary = np.sin(4 * x1) + 1

y = (x2 > boundary).astype(int)

df_knn = pd.DataFrame({
    "x1": x1,
    "x2": x2,
    "y": y
})

df_knn.head()
```
```{python}
#| echo: true
#| output: true
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

x1_range = np.linspace(-3, 3, 200)
x2_range = np.linspace(-3, 3, 200)
xx1, xx2 = np.meshgrid(x1_range, x2_range)
grid_points = np.c_[xx1.ravel(), xx2.ravel()]

def plot_knn_boundary(k, ax):
    clf = KNeighborsClassifier(n_neighbors=k)
    clf.fit(df_knn[["x1", "x2"]], df_knn["y"])
    preds = clf.predict(grid_points).reshape(xx1.shape)
    
    ax.contourf(xx1, xx2, preds, cmap="Pastel1", alpha=0.8)
    ax.scatter(df_knn["x1"], df_knn["x2"], c=df_knn["y"], edgecolor="k", cmap="coolwarm", s=30)
    ax.set_title(f"KNN (k={k})")
    ax.set_xlabel("x1")
    ax.set_ylabel("x2")

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for i, k in enumerate([1, 5, 15]):
    plot_knn_boundary(k, axes[i])

plt.tight_layout()
plt.show()
```

### 2a. K-Nearest Neighbors: Visualizing Decision Boundaries

Using a synthetic dataset defined by a sine-based nonlinear boundary, we fit KNN classifiers with varying values of k.

- **k = 1**: The model overfits, producing highly irregular decision boundaries.
- **k = 5**: The model smooths the boundary while retaining flexibility to capture nonlinearity.
- **k = 15**: The model is very smooth, but may underfit by failing to capture finer structure.

This experiment highlights the bias-variance tradeoff in KNN: small k yields low bias but high variance, while large k smooths the prediction surface but may miss important detail.
```{python}
#| echo: true
#| output: true
import matplotlib.pyplot as plt

np.random.seed(123)
n_test = 100
x1_test = np.random.uniform(-3, 3, n_test)
x2_test = np.random.uniform(-3, 3, n_test)
boundary_test = np.sin(4 * x1_test) + 1
y_test = (x2_test > boundary_test).astype(int)

df_test = pd.DataFrame({
    "x1": x1_test,
    "x2": x2_test,
    "y": y_test
})

plt.figure(figsize=(6, 5))
plt.scatter(df_knn["x1"], df_knn["x2"], c=df_knn["y"], cmap="coolwarm", s=40, edgecolor='k', label="Train")
plt.scatter(df_test["x1"], df_test["x2"], c=df_test["y"], cmap="coolwarm", s=40, marker="^", edgecolor='k', label="Test")
plt.plot(x1_range, np.sin(4 * x1_range) + 1, color="black", linestyle="--", label="True Boundary")
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Train/Test Data with Wiggly Boundary")
plt.legend()
plt.grid(True)
plt.show()
```

### Visualizing Training and Test Data with True Boundary

We generated a synthetic dataset where a binary label `y` depends on whether `x2 > sin(4x1) + 1`.

- The **training data** is shown as crosses (`×`), and
- The **test data** is shown as triangles (`▲`).
- The **true decision boundary**, defined by `x2 = sin(4x1) + 1`, is drawn as a black dashed line.

This setup creates a highly nonlinear classification boundary, which challenges traditional linear models and motivates the use of KNN and other nonparametric methods.
```{python}
#| echo: true
#| output: true
def knn_predict(X_train, y_train, X_test, k):
    predictions = []
    for x in X_test:
        distances = np.linalg.norm(X_train - x, axis=1)
        nearest = np.argsort(distances)[:k]
        votes = y_train[nearest]
        pred = np.round(np.mean(votes)).astype(int)
        predictions.append(pred)
    return np.array(predictions)

X_train = df_knn[["x1", "x2"]].values
y_train = df_knn["y"].values
X_test = df_test[["x1", "x2"]].values
y_test = df_test["y"].values

y_pred_manual = knn_predict(X_train, y_train, X_test, k=5)

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
y_pred_sklearn = model.predict(X_test)

manual_match = np.mean(y_pred_manual == y_pred_sklearn)
manual_accuracy = np.mean(y_pred_manual == y_test)
sklearn_accuracy = np.mean(y_pred_sklearn == y_test)

{
    "Consistency (handwritten vs sklearn）": round(manual_match * 100, 2),
    "Handwritten KNN accuracy": round(manual_accuracy * 100, 2),
    "sklearn KNN accuracy": round(sklearn_accuracy * 100, 2)
}
```

```{python}
#| echo: true
#| output: true
accuracy_by_k = []

for k in range(1, 31):
    y_pred = knn_predict(X_train, y_train, X_test, k)
    acc = np.mean(y_pred == y_test)
    accuracy_by_k.append(acc)

plt.figure(figsize=(8, 5))
plt.plot(range(1, 31), np.array(accuracy_by_k) * 100, marker='o')
plt.xlabel("k (Number of Neighbors)")
plt.ylabel("Test Accuracy (%)")
plt.title("KNN Test Accuracy by k")
plt.grid(True)
plt.xticks(range(1, 31, 2))
plt.show()

best_k = np.argmax(accuracy_by_k) + 1
best_accuracy = accuracy_by_k[best_k - 1]

best_k, round(best_accuracy * 100, 2)
```

### Choosing the Optimal k for KNN

To select the optimal value of `k`, we evaluated KNN classifiers for values of `k` from 1 to 30. The accuracy on a separate test set was recorded for each model.

The resulting plot shows that:
- Accuracy peaked at **k = 1**, achieving **87.0% accuracy** on the test set.
- Larger k values slightly smoothed performance but did not outperform k = 1.

This result is consistent with the nature of our synthetic data, where the decision boundary is highly nonlinear and local, allowing low-k models to perform well.

```{python}
#| echo: true
#| output: true
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from scipy.stats import pearsonr

df_driver = pd.read_csv("data_for_drivers_analysis.csv")

target_col = "brand"
feature_cols = ['satisfaction', 'trust', 'build', 'differs', 'easy',
                'appealing', 'rewarding', 'popular', 'service', 'impact']

X = df_driver[feature_cols].values
y = df_driver[target_col].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pearson_scores = [abs(pearsonr(X[:, i], y)[0]) for i in range(X.shape[1])]

linreg = LinearRegression().fit(X_scaled, y)
regression_coefs = abs(linreg.coef_)

full_r2 = r2_score(y, linreg.predict(X_scaled))
usefulness_scores = []

for i in range(X.shape[1]):
    X_partial = np.delete(X_scaled, i, axis=1)
    model = LinearRegression().fit(X_partial, y)
    r2_partial = r2_score(y, model.predict(X_partial))
    usefulness_scores.append(full_r2 - r2_partial)

df_importance = pd.DataFrame({
    "Variable": feature_cols,
    "Pearson": pearson_scores,
    "Standardized_Regression": regression_coefs,
    "Usefulness_R2_Drop": usefulness_scores
}).round(4)

df_importance = df_importance.sort_values("Usefulness_R2_Drop", ascending=False).reset_index(drop=True)
df_importance
```

```{python}
#| echo: true
#| output: true
import shap

explainer = shap.Explainer(linreg, X_scaled)
shap_values = explainer(X_scaled)

shap_importance = np.abs(shap_values.values).mean(axis=0)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)
rf_importance = rf_model.feature_importances_

df_importance["Shapley_Value"] = shap_importance
df_importance["RF_Gini_Importance"] = rf_importance
df_importance = df_importance.round(4)
df_importance = df_importance.sort_values("Usefulness_R2_Drop", ascending=False).reset_index(drop=True)

df_importance.head(10)
```

### Key Drivers Analysis: Slide 75 Style Table

We computed six common variable importance metrics using the provided dataset, replicating the table from slide 75.

**Key takeaways**:
- `appealing` ranks highest in Usefulness (ΔR²) and Random Forest importance.
- `rewarding` has a dominant Shapley value, showing strong nonlinear effect.
- `easy` and `build` consistently rank high across regression-based and tree-based measures.
- `trust` shows low predictive power across all methods, despite appearing conceptually relevant.

These results provide a comprehensive view of which perception metrics drive brand preference, balancing linear and non-linear model perspectives.

```{python}
#| echo: true
#| output: true
import matplotlib.pyplot as plt
import seaborn as sns

heatmap_data = df_importance.copy()
heatmap_data_percent = heatmap_data.iloc[:, 1:] * 100

heatmap_data_percent.index = df_importance["Variable"]

plt.figure(figsize=(10, 6))
sns.heatmap(
    heatmap_data_percent,
    annot=True,
    fmt=".1f",
    cmap="Greens",
    linewidths=0.5,
    cbar_kws={"label": "Importance (%)"}
)
plt.title("Key Drivers Analysis Heatmap (Slide 75 Style)")
plt.ylabel("Perception Variables")
plt.tight_layout()
plt.show()
```

### Summary

We successfully replicated the importance metrics table using five different methods. Johnson's relative weights were not computed due to package limitations in Python, but all other key drivers metrics were covered, including:

- Correlation-based (Pearson)
- Regression-based (Standardized Coefficients, Usefulness)
- Model-based (Shapley value, Random Forest Gini)

These metrics provide a comprehensive view of which perception factors best explain brand choice.
